{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1117\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from collections import defaultdict\n",
    "from einops import rearrange, repeat\n",
    "from einops.layers.torch import Rearrange\n",
    "\n",
    "import h5py\n",
    "import json\n",
    "\n",
    "import math\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from random import shuffle,randint\n",
    "\n",
    "from scipy.fftpack import fft\n",
    "import scipy.stats as stats\n",
    "import scipy.io as scio\n",
    "from scipy.io import wavfile\n",
    "from scipy import signal\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "from torchmetrics.functional import word_error_rate as wer\n",
    "from textwrap import wrap\n",
    "import textgrid as tg\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from torch import nn, optim, einsum\n",
    "import torch.utils.data as Data\n",
    "from torch.utils.data import DataLoader,TensorDataset\n",
    "import wave\n",
    "\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "#device = 'cpu'\n",
    "date='1117'\n",
    "print(date)\n",
    "subject = 'PA4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nEXAMPLE:\\necog_mat,back_duration,forward_duration,mat = read_ecog_mat(back,forward,mat,channelNum,key_elecs=[],\\n                key_label=[],oppolabel=False,key_sentence=[],opposentence=False, key_paragraph=[],oppoparagraph=False,\\n                hz=False, block=4)\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#FUNCTIONS USED FOR LOAD DATA\n",
    "def get_timelocked_activity(times, hg, back, forward, hz=False):\n",
    "    '''\n",
    "    Get time-locked activity.\n",
    "\n",
    "    Parameters:\n",
    "    - times (array-like): List of timepoints in seconds.\n",
    "    - hg (numpy.ndarray): High gamma array, shaped (elecs_num, whole_duration*hz).\n",
    "    - back (float): Start time before the timepoints.\n",
    "    - forward (float): End time after the timepoints.\n",
    "    - hz (float, optional): Sampling rate of High gamma. Default is False.\n",
    "\n",
    "    Returns:\n",
    "    - Y_mat (numpy.ndarray): Time-locked activity array, shaped (trial_num, elecs_num, selected_duration*hz).\n",
    "    - back (int): Start time in samples (back*hz).\n",
    "    - forward (int): End time in samples (forward*hz).\n",
    "    '''\n",
    "    if hz:\n",
    "        times = (times*hz).astype(int)\n",
    "        back = int(back*hz)\n",
    "        forward = int(forward*hz)\n",
    "    times = times[times - back > 0]\n",
    "    times = times[times + forward < hg.shape[1]]\n",
    "\n",
    "    Y_mat = np.zeros((len(times),hg.shape[0], int(back + forward)), dtype=float)\n",
    "\n",
    "    for i, index in enumerate(times):\n",
    "        Y_mat[i, :, :] = hg[:, int(index-back):int(index+forward)]\n",
    "\n",
    "    return Y_mat,back,forward\n",
    "\n",
    "def read_ecog_mat(back,forward,mat,channelNum,key_elecs=[],key_label=[],oppolabel=False,key_sentence=[],opposentence=False, key_paragraph=[],oppoparagraph=False,\n",
    "                  hz=False, block=4):\n",
    "    '''\n",
    "    get ecog_mat based on defined parameters\n",
    "    \n",
    "    Parameters:\n",
    "    - mat : shaped(7,num_of_timepoints)\n",
    "    - hg (numpy.ndarray): High gamma array, shaped (elecs_num, whole_duration*hz).\n",
    "    - back (float): Start time before the timepoints.\n",
    "    - forward (float): End time after the timepoints.\n",
    "    - hz (float, optional): Sampling rate of High gamma. Default is False.\n",
    "    channelNum:number of electrodes 128 or 256\n",
    "    block:number of blocks included, usually from 1 to 4\n",
    "    key_elecs: selected elecs\n",
    "    kay_labels:selected labels, opplabel means oppose the key_label\n",
    "    sentence and paragraphs similarly\n",
    "    '''\n",
    "\n",
    "    key_elecs = np.array(key_elecs)\n",
    "    key_index = np.ones(len(mat[0,:])).astype('bool')\n",
    "    \n",
    "    sentence_index = np.ones(len(mat[0,:])).astype('bool')\n",
    "    label_index = np.ones(len(mat[0,:])).astype('bool')\n",
    "    paragraph_index = np.ones(len(mat[0,:])).astype('bool')  \n",
    "    \n",
    "    if key_label:\n",
    "        print('select label:'+str(key_label),end=' ')\n",
    "        label_index=~label_index\n",
    "        for i in key_label:\n",
    "            temp_index = mat[0,:] ==i\n",
    "            label_index = np.logical_or(label_index, temp_index)\n",
    "        if oppolabel:\n",
    "            print('oppo')\n",
    "            label_index=~label_index\n",
    "    key_index = np.logical_and(label_index, key_index)\n",
    "    if key_sentence:\n",
    "        print('select sentence:'+str(key_sentence),end=' ')\n",
    "        sentence_index=~sentence_index\n",
    "        for i in key_sentence:\n",
    "            temp_index = mat[2,:] ==i\n",
    "            sentence_index = np.logical_or(sentence_index, temp_index)\n",
    "        if opposentence:\n",
    "            print('oppo')\n",
    "            sentence_index=~sentence_index\n",
    "    key_index = np.logical_and(sentence_index, key_index)        \n",
    "    if key_paragraph:\n",
    "        print('select paraqraph:'+str(key_paragraph),end=' ')\n",
    "        paragraph_index=~paragraph_index\n",
    "        for i in key_paragraph:\n",
    "            temp_index = mat[3,:] ==i\n",
    "            paragraph_index = np.logical_or(paragraph_index, temp_index)\n",
    "        if oppoparagraph:\n",
    "            print('oppo')\n",
    "            paragraph_index=~paragraph_index\n",
    "    key_index = np.logical_and(paragraph_index, key_index)      \n",
    "    mat = mat[:,key_index]\n",
    "       \n",
    "    \n",
    "    if key_elecs.any():\n",
    "        ecog_mat = np.zeros((len(mat[0,:]),len(key_elecs), int((back+forward)*hz)), dtype=float)\n",
    "        print('select elecs:'+str(key_elecs),end=' ')\n",
    "    else:\n",
    "        ecog_mat = np.zeros((len(mat[0,:]),channelNum, int((back+forward)*hz)), dtype=float)\n",
    "        \n",
    "    for i in range(block):\n",
    "        block_index= mat[4,:]==i+1\n",
    "        temp_time_list=mat[1,block_index]\n",
    "        ecogData=scio.loadmat(path_raw+str(i+1)+filterType+'.mat')#################################\n",
    "        ecogData=np.array(ecogData['bands'])*10e4\n",
    "        #print('###',np.nanmean(np.max(ecogData,axis=1)-np.min(ecogData,axis=1)),np.array(np.max(ecogData,axis=1)).shape)######################################################################################\n",
    "        temp_ecog,back_duration,forward_duration = get_timelocked_activity(times=temp_time_list, hg=ecogData, back=back, forward=forward, hz=hz)\n",
    "        if key_elecs.any():\n",
    "            key_elecs = np.array(key_elecs)\n",
    "            ecog_mat[block_index] = temp_ecog[:,key_elecs,:]\n",
    "        else:\n",
    "            ecog_mat[block_index] = temp_ecog\n",
    "\n",
    "    print('read_ecog:',ecog_mat.shape)\n",
    "    return ecog_mat,back_duration,forward_duration,mat\n",
    "\n",
    "'''\n",
    "EXAMPLE:\n",
    "ecog_mat,back_duration,forward_duration,mat = read_ecog_mat(back,forward,mat,channelNum,key_elecs=[],\n",
    "                key_label=[],oppolabel=False,key_sentence=[],opposentence=False, key_paragraph=[],oppoparagraph=False,\n",
    "                hz=False, block=4)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#FUNCTIONS USED FOR TRAIN AND VALIDATE THE MODEL\n",
    "def CV_datasets(back,forward,mat,key_elecs,row,CV_list,unbalance=False,List=False,augmented=False):\n",
    "    CV_datasets = []\n",
    "    CV_augmented_datasets=[]\n",
    "    count_total=[]\n",
    "    #split data to cross validation combinations, and calculate the weight of each labels\n",
    "    for CVs in range(6):\n",
    "        print('mat:',mat.shape)\n",
    "        test_ecog_mat,_,_,test_mat= read_ecog_mat(back=back,forward=forward,mat=mat,channelNum=channelNum,key_elecs=key_elecs,\n",
    "                key_label=[],oppolabel=False,key_sentence=[],opposentence=False, key_paragraph=CV_list[CVs],oppoparagraph=False,\n",
    "                hz=hz, block=4)\n",
    "\n",
    "        test_label = test_mat[row,:]\n",
    "        \n",
    "        if unbalance == True:\n",
    "            count=np.bincount(test_label.astype('int'))\n",
    "            if len(count)!=len(List):\n",
    "                print('not a full set!')\n",
    "                dataPlus1 = np.concatenate((data,np.arange(len(List))))\n",
    "                count=np.bincount(dataPlus1)-1    \n",
    "                del dataPlus1\n",
    "            print(count)\n",
    "            count_total.append(count)\n",
    "        test_ecog_mat=torch.FloatTensor(test_ecog_mat)\n",
    "        test_label=torch.tensor(test_label,dtype=torch.long)\n",
    "        testdataset = Data.TensorDataset(test_ecog_mat, test_label)    \n",
    "        CV_datasets.append(testdataset)\n",
    "        \n",
    "\n",
    "        if augmented is not False:\n",
    "            aug_mat = stammer (mat=test_mat, multiple_time=augmented,multiple_range=0.1,shift=-0.05)\n",
    "            aug_ecog_mat,_,_,test_mat= read_ecog_mat(back=back,forward=forward,mat=mat,channelNum=channelNum,key_elecs=key_elecs,\n",
    "                key_label=[],oppolabel=False,key_sentence=[],opposentence=False, key_paragraph=CV_list[CVs],oppoparagraph=False,\n",
    "                hz=hz, block=4)\n",
    "            aug_label = aug_mat[row,:]\n",
    "            aug_ecog_mat=torch.FloatTensor(aug_ecog_mat)\n",
    "            aug_label=torch.tensor(aug_label,dtype=torch.long)\n",
    "            augdataset = Data.TensorDataset(aug_ecog_mat, aug_label) \n",
    "            CV_augmented_datasets.append(augdataset)\n",
    "    if unbalance == True:\n",
    "        count_total = np.array(count_total)\n",
    "        \n",
    "    return CV_datasets,count_total,CV_augmented_datasets\n",
    "\n",
    "'''\n",
    "EXAMPLES:\n",
    "CV_onset_datasets,count_total,_ = CV_datasets(back=overt_back,\n",
    "                                            forward=overt_forward,\n",
    "                                            mat=overt_mat,\n",
    "                                            key_elecs=overt_elecs,\n",
    "                                            row=0,\n",
    "                                            CV_list,\n",
    "                                            unbalance=True,List=stateList,\n",
    "                                            augmented=False)\n",
    "class_weight = np.sum(count_total,axis=0)\n",
    "'''\n",
    "'''\n",
    "CV_sylb_datasets,_,_ = CV_datasets(back=sylb_back,\n",
    "                            forward=sylb_forward,\n",
    "                            mat=sylb_mat,\n",
    "                            key_elecs=sylb_elecs,\n",
    "                            row=0,\n",
    "                            CV_list,\n",
    "                            unbalance=False,List=False,\n",
    "                            augmented=False)\n",
    "'''\n",
    "'''\n",
    "CV_clip_datasets,_,_ = CV_datasets(back=sylb_back,\n",
    "                            forward=sylb_forward,\n",
    "                            mat=clipTimeMat,\n",
    "                            key_elecs=sylb_elecs,\n",
    "                            row=0,\n",
    "                            CV_list,\n",
    "                            unbalance=False,List=False,\n",
    "                            augmented=False)\n",
    "\n",
    "'''\n",
    "def CV_train_onset(CV_datasets, lr, batch_size, patience, class_weight, val_ratio, channelNum=256):\n",
    "    loss_func = nn.CrossEntropyLoss()\n",
    "    #This function used to preform cross validation\n",
    "    acc_list=[]\n",
    "    loss_list=[]\n",
    "    label_list=[]\n",
    "    test_out_list=[]\n",
    "    test_out_prob_list=[]\n",
    "\n",
    "    for test_CV in range(6):#tqdm(\n",
    "\n",
    "        CV_train_val_datasets = CV_datasets.copy()\n",
    "        CV_test_datasets = [CV_train_val_datasets.pop(test_CV),]\n",
    "        \n",
    "        CV_train_datasets = []\n",
    "        CV_val_datasets = []\n",
    "        for train_val_dataset in CV_train_val_datasets:\n",
    "            val_size = int(val_ratio * len(train_val_dataset))\n",
    "            train_size = len(train_val_dataset) - val_size\n",
    "            train_dataset, val_dataset = Data.random_split(train_val_dataset, [train_size, val_size])\n",
    "            CV_train_datasets.append(train_dataset)\n",
    "            CV_val_datasets.append(val_dataset)\n",
    "        #label,acc,predicted,predicted_prob, predicted_only,predicted_only_prob,model\n",
    "        label,_,_,predicted_prob,_,_,_,loss_func = train( \n",
    "                                        lr=lr, batch_size=batch_size, EPOCH=EPOCH, patience=patience, \n",
    "                                        CV_train_datasets = CV_train_datasets, #CV_train_val_datasets, #\n",
    "                                        CV_val_datasets =  CV_val_datasets, #False, #\n",
    "                                        CV_test_datasets = CV_test_datasets,#False, \n",
    "                                        pred_only_datasets = False, #pred_only_datasets, \n",
    "                                        class_weight=class_weight)\n",
    "            \n",
    "        #predicted_prob_list.append(predicted_prob)\n",
    "        #predicted_prob = np.mean(predicted_prob_list, axis=0)\n",
    "        \n",
    "        loss = loss_func(torch.tensor(predicted_prob), torch.tensor(label))\n",
    "        loss = loss / len(label)\n",
    "        predicted = np.argmax(predicted_prob, axis=1)\n",
    "        correct = np.sum(predicted == np.array(label))\n",
    "        acc = correct / len(label)\n",
    "        #tune.track.log(loss=loss)####################################################\n",
    "       \n",
    "        \n",
    "        acc_list.append(acc)\n",
    "        loss_list.append(loss)\n",
    "        label_list.extend(label)\n",
    "        test_out_list.extend(predicted) \n",
    "        test_out_prob_list.extend(predicted_prob) \n",
    "        if verbose:\n",
    "            plotCM(List = stateList,test_acc_list=acc_list,test_label_list=label_list,test_out_list=test_out_list)\n",
    "        \n",
    "    return acc_list, loss_list, label_list, test_out_prob_list\n",
    "'''\n",
    "_, _, label_list, onset_prob = CV_train_onset(CV_datasets=CV_onset_datasets, \n",
    "                                    lr, batch_size, patience, \n",
    "                                    class_weight, val_ratio, \n",
    "                                    channelNum=256)\n",
    "'''\n",
    "def train(batch_size, lr, EPOCH, patience, CV_train_datasets, \n",
    "          CV_val_datasets=False, CV_test_datasets=False, pred_only_datasets=False, \n",
    "          class_weight=False):\n",
    "    #This function directly train and validate the model\n",
    "    model = torch.load(\"./\"+subject+'.pt')\n",
    "    loss_func = nn.CrossEntropyLoss() \n",
    "    if class_weight is not False:\n",
    "        weight = [max(class_weight)/x for x in class_weight]\n",
    "        weight = torch.FloatTensor(weight).to(device)\n",
    "        loss_func = nn.CrossEntropyLoss(weight=weight)\n",
    "        \n",
    "    optimizer = torch.optim.Adam(model.parameters(),lr=lr)#也可以使用SGD优化算法进行训练\n",
    "    \n",
    "    early_stopping = EarlyStopping(patience)\n",
    "    '''\n",
    "    optimizer = torch.optim.SGD(model.parameters(),lr=lr)#也可以使用SGD优化算法进行训练\n",
    "    scheduler =  torch.optim.lr_scheduler.CosineAnnealingLR(optimizer = optimizer,T_max =  EPOCH)\n",
    "    \n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=patience, \n",
    "                                           verbose=False, threshold=0.0001, threshold_mode='rel', \n",
    "                                           cooldown=0, min_lr=0, eps=1e-08)\n",
    "    '''\n",
    "    for epoch in range(EPOCH):\n",
    "        #return acc，ground truth & predicited label\n",
    "        sum_loss_train= 0\n",
    "        total_train=0\n",
    "        model.train()\n",
    "        for train_dataset in CV_train_datasets:\n",
    "            train_loader = Data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "            for data in train_loader:\n",
    "                inputs, labels = data\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                pred = model(inputs)\n",
    "                loss = loss_func(pred, labels)\n",
    "                sum_loss_train+= loss.item()\n",
    "                total_train+= labels.size(0)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "        train_loss = sum_loss_train/total_train\n",
    "        if verbose:\n",
    "            print('Epoch {}:train-loss:{:.2e},'.format(epoch+1,train_loss),end='')\n",
    "       \n",
    "        if CV_val_datasets is not False:#if there is val datasets performing early-stopping\n",
    "            sum_loss_val=0\n",
    "            total_val = 0  \n",
    "            for val_dataset in CV_val_datasets:\n",
    "                val_loader = Data.DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "                for data in val_loader:\n",
    "                    inputs, labels = data\n",
    "                    inputs, labels = inputs.to(device), labels.to(device) \n",
    "                    pred = model(inputs)\n",
    "                    loss = loss_func(pred, labels)\n",
    "                    sum_loss_val+=loss.item()\n",
    "                    total_val+= labels.size(0)\n",
    "            val_loss = sum_loss_val/total_val\n",
    "            if verbose:\n",
    "                print('val-loss:{:.2e},'.format(val_loss,),end='')\n",
    "            \n",
    "            '''\n",
    "            scheduler.step(val_loss)##############################################\n",
    "            #scheduler.step()\n",
    "            '''\n",
    "            early_stopping(val_loss, model)\n",
    "            if early_stopping.early_stop:\n",
    "                if verbose:\n",
    "                    print(\"Early stopping\")\n",
    "                break\n",
    "        \n",
    "        if CV_test_datasets is not False: #if there is test data for testing\n",
    "            total_test = 0  \n",
    "            sum_loss_test= 0\n",
    "            predicted=[]\n",
    "            label=[]\n",
    "            \n",
    "            for test_dataset in CV_test_datasets:\n",
    "                test_loader = Data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "                for data in test_loader:\n",
    "                    inputs, labels = data\n",
    "                    inputs, labels = inputs.to(device), labels.to(device)\n",
    "                    pred_prob = model(inputs)\n",
    "                    loss = loss_func(pred_prob, labels)\n",
    "                    sum_loss_test+=loss.item()\n",
    "                    _, pred = torch.max(pred_prob.data, 1)\n",
    "                    pred_prob = pred_prob.data\n",
    "                    pred_prob = F.softmax(pred_prob, dim=1) \n",
    "                    total_test+= labels.size(0)\n",
    "                    predicted.append(pred.cpu())\n",
    "                    label.append(labels.cpu())\n",
    "            label = torch.cat(label,dim=0)\n",
    "            predicted = torch.cat(predicted,dim=0)\n",
    "            correct = (predicted == label).sum()\n",
    "            test_acc= correct.item()/total_test\n",
    "            test_loss=sum_loss_test/total_test\n",
    "            if verbose:\n",
    "                print('test-loss:{:.2e},Acc:{:.4f}'.format(test_loss,test_acc),end='')\n",
    "        if verbose:\n",
    "            print('')\n",
    "    \n",
    "    model.load_state_dict(torch.load('checkpoint.pt'))\n",
    "    \n",
    "    ##############################EVALUATION model###############################################\n",
    "    predicted=[]\n",
    "    predicted_prob=[]\n",
    "    label=[]\n",
    "    acc=[]\n",
    "    if CV_test_datasets is not False:\n",
    "        total_test = 0  # 总数\n",
    "        sum_loss_test= 0\n",
    "        model.eval()\n",
    "        for test_dataset in CV_test_datasets:\n",
    "            test_loader = Data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "            for data in test_loader:\n",
    "                inputs, labels = data\n",
    "                inputs, labels = inputs.to(device), labels.to(device)  # 有GPU则将数据置入GPU加速\n",
    "                pred_prob = model(inputs)\n",
    "                loss = loss_func(pred_prob, labels)\n",
    "                sum_loss_test+=loss.item()\n",
    "                _, pred = torch.max(pred_prob.data, 1)\n",
    "                pred_prob = pred_prob.data\n",
    "                pred_prob = F.softmax(pred_prob, dim=1) \n",
    "                total_test+= labels.size(0)\n",
    "\n",
    "                predicted.append(pred.cpu())\n",
    "                predicted_prob.append(pred_prob.cpu())\n",
    "                label.append(labels.cpu())\n",
    "\n",
    "        label = torch.cat(label,dim=0)\n",
    "        predicted = torch.cat(predicted,dim=0)\n",
    "        predicted_prob = torch.cat(predicted_prob,dim=0)\n",
    "        correct = (predicted == label).sum()\n",
    "        test_acc= correct.item()/total_test\n",
    "        test_loss=sum_loss_test/total_test\n",
    "        print('Final: test-loss:{:.2e},Acc:{:.4f}'.format(test_loss,test_acc))\n",
    "        label = label.detach().numpy()\n",
    "        acc = test_acc\n",
    "        predicted = predicted.detach().numpy()\n",
    "        predicted_prob = predicted_prob.detach().numpy()\n",
    "    \n",
    "    \n",
    "    ##PRED-ONLY:do not return acc, only predicted result\n",
    "    predicted_only=[]\n",
    "    predicted_only_prob=[]\n",
    "        \n",
    "    if pred_only_datasets is not False:\n",
    "        for pred_only_dataset in pred_only_datasets:\n",
    "            pred_only_loader = Data.DataLoader(pred_only_dataset, batch_size=batch_size, shuffle=False)\n",
    "            for inputs in pred_only_loader:\n",
    "                inputs = inputs[0].to(device)\n",
    "                pred = model(inputs)\n",
    "                pred_only_prob = pred.data\n",
    "                pred_only_prob = F.softmax(pred_only_prob, dim=1) \n",
    "                _, pred_only = torch.max(pred.data, 1)\n",
    "                predicted_only.append(pred_only.cpu())\n",
    "                predicted_only_prob.append(pred_only_prob.cpu())\n",
    "            \n",
    "        predicted_only = torch.cat(predicted_only).detach().numpy()\n",
    "        predicted_only_prob = torch.cat(predicted_only_prob).detach().numpy()\n",
    "        print(predicted_only_prob.shape,'#$#')\n",
    "    \n",
    "    return label,acc, predicted,predicted_prob, predicted_only,predicted_only_prob, model,loss_func.cpu()\n",
    "\n",
    "class EarlyStopping:\n",
    "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
    "    def __init__(self, patience=7, delta=0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): How long to wait after last time validation loss improved.\n",
    "                            Default: 7\n",
    "            verbose (bool): If True, prints a message for each validation loss improvement. \n",
    "                            Default: False\n",
    "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
    "                            Default: 0\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.delta = delta\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                print(f'EarlyStopping: {self.counter}/{self.patience}',end='')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        '''Saves model when validation loss decrease.'''\n",
    "        #if self.verbose:\n",
    "            #print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}). save model ...')\n",
    "        torch.save(model.state_dict(), 'checkpoint.pt')\t# save the best model\n",
    "        self.val_loss_min = val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nEXAMPLE\\nplot = True\\nF1 = seq_F1_score(interval,onset_label_mat, predicted = np.vstack(onset_prob)[:,1], plot=True,\\n                    wd=int(0.018*hz),smooth_threshold=0.81,onset_duration=0.02,offset_duration=0.08,\\n                    word_duration = 0.5, eps = 0.08)\\nprint(F1)\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#FUNCTIONS USED FOR EVALUATION THE PERFORMANCE AND VISUALIZATION\n",
    "def plotCM(List,test_acc_list,test_label_list,test_out_list,wer_list=False):\n",
    "    # plot confusion matrix\n",
    "    C2= confusion_matrix(test_label_list, test_out_list,normalize='true')#,\n",
    "    #C2= confusion_matrix(test_out, predicted,normalize='true')#test_out->\n",
    "    labels=List\n",
    "    print(C2) \n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix = C2, display_labels=labels)\n",
    "    fig=plt.figure()\n",
    "    fig.set_size_inches(200,200)\n",
    "    disp.plot()\n",
    "    plt.title('N='+str(len(test_acc_list))+\"  Acc:%0.2f±%0.2f\" % (np.mean(test_acc_list),np.std(test_acc_list)))\n",
    "    if save:\n",
    "        plt.savefig(stage+'_CM.png', bbox_inches='tight')\n",
    "    return 0\n",
    "\n",
    "def seq_F1_score(interval, test_mat, predicted, plot=True,\n",
    "           wd=20,smooth_threshold=0.6,onset_duration=0.08,offset_duration=0.08,word_duration = 0.8,eps = 0.05):\n",
    "    # caluculate the F1_score as the performance for hyperopt \n",
    "    label = test_mat[0,:]\n",
    "    hwd=int(wd/2)\n",
    "    smooth_out=predicted.copy()\n",
    "    for i in range(hwd,len(predicted)-hwd+1):\n",
    "        smooth_out[i]=np.nanmean(predicted[i-hwd:i+hwd])\n",
    "    bi_out=smooth_out>smooth_threshold\n",
    "    \n",
    "    onset_predicted=np.zeros((bi_out.shape))   \n",
    "    onset_label=np.zeros((bi_out.shape))\n",
    "        \n",
    "    clipTimeIndex=[]\n",
    "    stableOnset = onset_duration//interval\n",
    "    stableOffset = offset_duration//interval\n",
    "    sayAword=word_duration//interval\n",
    "\n",
    "    i = int(stableOnset)\n",
    "    while i < len(bi_out)-stableOnset:\n",
    "        if bi_out[i]==1 and bi_out[i-1]==0:\n",
    "            if np.mean(bi_out[i:int(i+stableOnset)])>1-eps:\n",
    "                if np.mean(bi_out[int(i-stableOffset):i])<eps:\n",
    "                    onset_predicted[i-100:i+100] = 1\n",
    "                    clipTimeIndex.append(i)\n",
    "                    i+=int(sayAword-1)\n",
    "        i+=1\n",
    "    \n",
    "    i = int(stableOnset)\n",
    "    while i < len(label)-stableOnset:\n",
    "        if label[i]==1 and label[i-1]==0:\n",
    "            if np.mean(label[i:int(i+stableOnset)])>1-eps:\n",
    "                if np.mean(label[int(i-stableOffset):i])<eps:\n",
    "                    onset_label[i-100:i+100] = 1 ####误差+-0.25s内\n",
    "                    i+=int(sayAword-1)\n",
    "        i+=1    \n",
    "        \n",
    "    from sklearn.metrics import accuracy_score\n",
    "    ACC = accuracy_score(onset_label, onset_predicted)   \n",
    "    \n",
    "    if plot:\n",
    "        clipTimeIndex = np.array(clipTimeIndex)\n",
    "        clipTimeMat = test_mat[:,clipTimeIndex]\n",
    "        trial_values = set(test_mat[3,:])\n",
    "        print (trial_values)\n",
    "        fig, axs = plt.subplots(len(trial_values), 1, figsize=(500, len(trial_values) *3))\n",
    "        # Iterate over each split timescale and plot it as a subplot\n",
    "        for i, index in enumerate(trial_values):\n",
    "            current_index = [j for j, val in enumerate(test_mat[3,:]) if val == index]\n",
    "            current_timescale = [test_mat[1,j] for j in current_index]\n",
    "            #print(len(current_timescale))\n",
    "            current_predicted = [predicted[j] for j in current_index]\n",
    "            #print(len(current_predicted))\n",
    "            current_label = [test_mat[0,j] for j in current_index]\n",
    "            current_smooth_out = [smooth_out[j] for j in current_index]\n",
    "            current_bi_out = [bi_out[j] for j in current_index]\n",
    "            \n",
    "            current_split_index = [t for t, val in enumerate(clipTimeMat[3,:]) if val == index]\n",
    "            current_clip_time = [clipTimeMat[1,j] for j in current_split_index]\n",
    "            \n",
    "            axs[i].set_xlim(min(current_timescale), max(current_timescale))\n",
    "            axs[i].plot(current_timescale, current_predicted, 'r+')\n",
    "            axs[i].plot(current_timescale, current_smooth_out, 'y+')\n",
    "            axs[i].plot(current_timescale, current_bi_out, 'k+')\n",
    "            axs[i].plot(current_timescale, current_label, 'b+')\n",
    "\n",
    "            for T in current_clip_time:\n",
    "                #axs[i].axvline(T, color='green')\n",
    "                axs[i].text(T, 0.1, f'{T:.2f}', color='green', ha='center', va='bottom')\n",
    "            axs[i].set_xticks(current_clip_time)\n",
    "            axs[i].tick_params(axis='x', which='major', bottom=True, top=True, direction='in', length=100, width=1, color='green')\n",
    "            axs[i].set_title('Trial ' + str(index))\n",
    "\n",
    "            \n",
    "            \n",
    "        # Adjust the spacing between subplots\n",
    "        plt.tight_layout()\n",
    "\n",
    "        # Save the figure\n",
    "        if save:\n",
    "            plt.savefig('view_onset.png')\n",
    "        \n",
    "    return ACC\n",
    "\n",
    "'''\n",
    "EXAMPLE\n",
    "plot = True\n",
    "F1 = seq_F1_score(interval,onset_label_mat, predicted = np.vstack(onset_prob)[:,1], plot=True,\n",
    "                    wd=int(0.018*hz),smooth_threshold=0.81,onset_duration=0.02,offset_duration=0.08,\n",
    "                    word_duration = 0.5, eps = 0.08)\n",
    "print(F1)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#FUNCTIONS FOR HYPERPARAMETERS TUNING\n",
    "def hyperopt_onset(overt_back = 0.25,\n",
    "                   overt_forward = 0.25,\n",
    "                   gruDim = 3,\n",
    "                   gruLayer = 64,\n",
    "                   drop_out = 0.5,\n",
    "                   smooth_window = 0.042,\n",
    "                   smooth_threshold = 0.76, \n",
    "                   onset_duration = 0.02,\n",
    "                   offset_duration = 0.02,\n",
    "                   eps = 0.05,\n",
    "                   batch_size = 4096, \n",
    "                   lr = 0.0005):    \n",
    "    outPutLoss = 9999 \n",
    "    F1 = 0\n",
    "    \n",
    "    F1 = seq_F1_score(interval, onset_label_mat, predicted = np.vstack(onset_prob)[:,1], plot=False,\n",
    "                    wd=int(smooth_window*hz),\n",
    "                    smooth_threshold = smooth_threshold,\n",
    "                    onset_duration = onset_duration,\n",
    "                    offset_duration = offset_duration,\n",
    "                    word_duration = 0.5, \n",
    "                    eps = eps)\n",
    "    \n",
    "    a = np.array([outPutLoss,F1,\n",
    "                  overt_back, overt_forward, gruDim, gruLayer, drop_out,\n",
    "                  smooth_window, smooth_threshold, onset_duration, offset_duration, eps,\n",
    "                  batch_size, lr])\n",
    "    if verbose:\n",
    "        print(a)\n",
    "    reporterList.append(a)\n",
    "    #np.savetxt(\"hyperoptReporter.csv\", reporterList, delimiter = \",\", fmt = '%s')\n",
    "    \n",
    "    return outPutLoss, F1\n",
    "\n",
    "\n",
    "'''\n",
    "EXAMPLE:\n",
    "reporterList = []\n",
    "print(hyperopt_onset(overt_back = 0.25,\n",
    "                   overt_forward = 0.25,\n",
    "                   gruDim = 3,\n",
    "                   gruLayer = 64,\n",
    "                   drop_out = 0.5,\n",
    "                   smooth_window = 0.042,\n",
    "                   smooth_threshold = 0.76, \n",
    "                   onset_duration = 0.02,\n",
    "                   offset_duration = 0.02,\n",
    "                   batch_size = 4096, \n",
    "                   lr = 0.0005))\n",
    "'''\n",
    "from hyperopt import hp,STATUS_OK,Trials,fmin,tpe\n",
    "def hyperopt_train(params):\n",
    "    loss, F1 =hyperopt_onset(**params)\n",
    "    loss = -F1\n",
    "    return loss\n",
    "def f(params):\n",
    "    try:\n",
    "        loss = hyperopt_train(params)\n",
    "    except Exception as e:\n",
    "        # Handle the exception here (e.g., print the error message, hyperparas conflicting)\n",
    "        print(f\"Error: {e}\")\n",
    "        # Assign a high loss value to discourage selecting this parameter combination\n",
    "        return {'loss': 9999, 'status': STATUS_OK}\n",
    "    \n",
    "    return {'loss': loss, 'status': STATUS_OK}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 178692)\n",
      "select paraqraph:[13, 14, 15, 16] oppo\n",
      "select elecs:[1] read_ecog: (137374, 1, 8)\n",
      "(5, 137374)\n",
      "(7, 1034)\n",
      "select paraqraph:[13, 14, 15, 16] oppo\n",
      "select elecs:[1] read_ecog: (775, 1, 8)\n",
      "[[  8.        5.        4.      ...   2.        1.        0.     ]\n",
      " [ 24.2053   25.07445  26.36158 ... 532.05439 533.36673 534.45248]\n",
      " [  1.        1.        1.      ...   7.        7.        7.     ]\n",
      " ...\n",
      " [  1.        1.        1.      ...   3.        3.        3.     ]\n",
      " [  0.        0.        3.      ...   3.        1.        0.     ]\n",
      " [ 32.       21.       19.      ...  12.        7.        0.     ]]\n",
      "mat: (5, 178692)\n",
      "select paraqraph:[1, 2] select elecs:[  0   1   2   5   8   9  10  12  13  14  15  29  30  31  32  45  46  48\n",
      "  49  53  59  61  64  68  69  75  78  80  94  95  96 100 107 110 111 112\n",
      " 113 114 126 127 128 130 131 133 134 135 136 137 138 139 140 142 143 150\n",
      " 151 152 153 154 155 156 157 158 159 165 166 167 168 169 170 171 172 173\n",
      " 174 175 180 181 182 183 184 185 186 187 188 189 190 191 195 197 198 199\n",
      " 200 202 204 205 206 211 212 213 214 215 217 219 221 222 223 224 225 226\n",
      " 227 228 229 230 231 232 234 236 237 238 239 242 244 245 246 247 251 252\n",
      " 253 254 255] read_ecog: (22339, 129, 200)\n",
      "[16051  6288]\n",
      "mat: (5, 178692)\n",
      "select paraqraph:[3, 4] select elecs:[  0   1   2   5   8   9  10  12  13  14  15  29  30  31  32  45  46  48\n",
      "  49  53  59  61  64  68  69  75  78  80  94  95  96 100 107 110 111 112\n",
      " 113 114 126 127 128 130 131 133 134 135 136 137 138 139 140 142 143 150\n",
      " 151 152 153 154 155 156 157 158 159 165 166 167 168 169 170 171 172 173\n",
      " 174 175 180 181 182 183 184 185 186 187 188 189 190 191 195 197 198 199\n",
      " 200 202 204 205 206 211 212 213 214 215 217 219 221 222 223 224 225 226\n",
      " 227 228 229 230 231 232 234 236 237 238 239 242 244 245 246 247 251 252\n",
      " 253 254 255] read_ecog: (22831, 129, 200)\n",
      "[16699  6132]\n",
      "mat: (5, 178692)\n",
      "select paraqraph:[5, 6] select elecs:[  0   1   2   5   8   9  10  12  13  14  15  29  30  31  32  45  46  48\n",
      "  49  53  59  61  64  68  69  75  78  80  94  95  96 100 107 110 111 112\n",
      " 113 114 126 127 128 130 131 133 134 135 136 137 138 139 140 142 143 150\n",
      " 151 152 153 154 155 156 157 158 159 165 166 167 168 169 170 171 172 173\n",
      " 174 175 180 181 182 183 184 185 186 187 188 189 190 191 195 197 198 199\n",
      " 200 202 204 205 206 211 212 213 214 215 217 219 221 222 223 224 225 226\n",
      " 227 228 229 230 231 232 234 236 237 238 239 242 244 245 246 247 251 252\n",
      " 253 254 255] read_ecog: (22886, 129, 200)\n",
      "[17554  5332]\n",
      "mat: (5, 178692)\n",
      "select paraqraph:[7, 8] select elecs:[  0   1   2   5   8   9  10  12  13  14  15  29  30  31  32  45  46  48\n",
      "  49  53  59  61  64  68  69  75  78  80  94  95  96 100 107 110 111 112\n",
      " 113 114 126 127 128 130 131 133 134 135 136 137 138 139 140 142 143 150\n",
      " 151 152 153 154 155 156 157 158 159 165 166 167 168 169 170 171 172 173\n",
      " 174 175 180 181 182 183 184 185 186 187 188 189 190 191 195 197 198 199\n",
      " 200 202 204 205 206 211 212 213 214 215 217 219 221 222 223 224 225 226\n",
      " 227 228 229 230 231 232 234 236 237 238 239 242 244 245 246 247 251 252\n",
      " 253 254 255] read_ecog: (22920, 129, 200)\n",
      "[16956  5964]\n",
      "mat: (5, 178692)\n",
      "select paraqraph:[9, 10] select elecs:[  0   1   2   5   8   9  10  12  13  14  15  29  30  31  32  45  46  48\n",
      "  49  53  59  61  64  68  69  75  78  80  94  95  96 100 107 110 111 112\n",
      " 113 114 126 127 128 130 131 133 134 135 136 137 138 139 140 142 143 150\n",
      " 151 152 153 154 155 156 157 158 159 165 166 167 168 169 170 171 172 173\n",
      " 174 175 180 181 182 183 184 185 186 187 188 189 190 191 195 197 198 199\n",
      " 200 202 204 205 206 211 212 213 214 215 217 219 221 222 223 224 225 226\n",
      " 227 228 229 230 231 232 234 236 237 238 239 242 244 245 246 247 251 252\n",
      " 253 254 255] read_ecog: (22978, 129, 200)\n",
      "[16619  6359]\n",
      "mat: (5, 178692)\n",
      "select paraqraph:[11, 12] select elecs:[  0   1   2   5   8   9  10  12  13  14  15  29  30  31  32  45  46  48\n",
      "  49  53  59  61  64  68  69  75  78  80  94  95  96 100 107 110 111 112\n",
      " 113 114 126 127 128 130 131 133 134 135 136 137 138 139 140 142 143 150\n",
      " 151 152 153 154 155 156 157 158 159 165 166 167 168 169 170 171 172 173\n",
      " 174 175 180 181 182 183 184 185 186 187 188 189 190 191 195 197 198 199\n",
      " 200 202 204 205 206 211 212 213 214 215 217 219 221 222 223 224 225 226\n",
      " 227 228 229 230 231 232 234 236 237 238 239 242 244 245 246 247 251 252\n",
      " 253 254 255] read_ecog: (23420, 129, 200)\n",
      "[16797  6623]\n"
     ]
    }
   ],
   "source": [
    "path_raw='../Raw/B'\n",
    "sylbList = ['shi', 'de', 'ji', 'li', 'bu', 'ge', 'qi', 'zhe', 'ta', 'zhi']\n",
    "stateList=['silent','speech']\n",
    "toneList = ['1','2','3','4']\n",
    "hz = 400\n",
    "filterType = '_70_150'\n",
    "interval = 0.01\n",
    "channelNum = 256\n",
    "\n",
    "overt_mat = np.load('../'+subject+'_overt_mat'+'.npy',allow_pickle=True)\n",
    "print(overt_mat.shape)\n",
    "_,_,_,onset_label_mat =  read_ecog_mat(0.01,0.01,overt_mat,channelNum=256,key_elecs=[1,],\n",
    "                key_label=[],oppolabel=False,key_sentence=[],opposentence=False, \n",
    "                key_paragraph=[13,14,15,16],oppoparagraph=True,hz=hz, block=4)\n",
    "print(onset_label_mat.shape)\n",
    "resp_elecs = np.load('../'+subject+'_resp_elecs.npy')\n",
    "\n",
    "\n",
    "sylb_mat = np.load('../'+subject+'_sylb_mat'+'.npy',allow_pickle=True)\n",
    "print(sylb_mat.shape)\n",
    "_,_,_,sylb_label_mat =  read_ecog_mat(0.01,0.01,sylb_mat,channelNum=256,key_elecs=[1,],\n",
    "                key_label=[],oppolabel=False,key_sentence=[],opposentence=False, \n",
    "                key_paragraph=[13,14,15,16],oppoparagraph=True,hz=hz, block=4)\n",
    "print(sylb_label_mat)\n",
    "\n",
    "CV_onset_datasets,count_total,_ = CV_datasets(back = 0.25,\n",
    "                                            forward = 0.25,\n",
    "                                            mat=overt_mat,\n",
    "                                            key_elecs=resp_elecs,\n",
    "                                            row=0,\n",
    "                                            CV_list=[[1,2],[3,4],[5,6],[7,8],[9,10],[11,12]],\n",
    "                                            unbalance=True,List=stateList,\n",
    "                                            augmented=False)\n",
    "#this is the weight of each type, use to generate weighted crossentrophy_loss function\n",
    "class_weight = np.sum(count_total,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#generate the speech detector model\n",
    "overt_back = 0.25\n",
    "overt_forward = 0.25\n",
    "val_ratio = 0.1 \n",
    "CV_list=[[1,2],[3,4],[5,6],[7,8],[9,10],[11,12]]\n",
    "batch_size = 2048\n",
    "lr=0.001\n",
    "EPOCH=50\n",
    "patience=10\n",
    "\n",
    "class CRNN(nn.Module):\n",
    "    def __init__(self, *, duration, typeNum, in_chans, \n",
    "                 num_layers=4, gruDim=256, drop_out=0.5):\n",
    "        super().__init__()\n",
    "        self.conv1d = nn.Conv1d(in_channels=in_chans, out_channels=gruDim, kernel_size=3, stride=1, padding=0)\n",
    "        self.leaky_relu = nn.LeakyReLU(negative_slope=0.01)\n",
    "        self.max_pooling = nn.MaxPool1d(kernel_size=2, stride=None, padding=0)\n",
    "        self.dropout = nn.Dropout(p=drop_out)\n",
    "        gru_layers = []\n",
    "        for i in range(num_layers):\n",
    "            if i == 0:\n",
    "                gru_layers.append(nn.GRU(gruDim, gruDim, 1, batch_first=True, bidirectional=True))\n",
    "            else:\n",
    "                gru_layers.append(nn.GRU(gruDim * 2, gruDim, 1, batch_first=True, bidirectional=True))\n",
    "            \n",
    "        # Create the sequential model with stacked GRU layers\n",
    "        self.gru_layers = nn.Sequential(*gru_layers)\n",
    "        #self.gru1 = nn.GRU(channelNum, gruDim, 3, batch_first=True, bidirectional=True)\n",
    "        elec_feature = int(2*gruDim)\n",
    "        self.fc1 = nn.Linear(elec_feature, typeNum)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #x = x.permute(0, 2, 1)  # Convert (batch_size, seq_len, input_size) to (batch_size, input_size, seq_len)\n",
    "        x = self.conv1d(x)\n",
    "        x = self.leaky_relu(x)\n",
    "        x = self.max_pooling(x)\n",
    "        x = rearrange(x,'batch electrodes duration -> batch duration electrodes')\n",
    "        for gru_layer in self.gru_layers:\n",
    "            x, _ = gru_layer(x)\n",
    "            #x = self.elu(x)\n",
    "            x = self.dropout(x)\n",
    "        \n",
    "        x = x[:, -1, :]\n",
    "        x = self.fc1(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "model1 = CRNN(duration = int((overt_back+overt_forward)*hz), typeNum = 2, \n",
    "             in_chans=len(resp_elecs), gruDim=256).to(device)\n",
    "\n",
    "torch.save(model1,(\"./\"+subject+'_onset.pt'))\n",
    "torch.save(model1,(\"./\"+subject+'.pt'))\n",
    "######"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:train-loss:2.41e-04,val-loss:2.67e-04,test-loss:1.79e-04,Acc:0.7785\n",
      "Epoch 2:train-loss:1.56e-04,val-loss:1.90e-04,test-loss:1.22e-04,Acc:0.8669\n",
      "Epoch 3:train-loss:1.14e-04,val-loss:1.80e-04,test-loss:1.12e-04,Acc:0.8736\n",
      "Epoch 4:train-loss:9.76e-05,val-loss:1.42e-04,test-loss:8.74e-05,Acc:0.9072\n",
      "Epoch 5:train-loss:1.05e-04,val-loss:2.02e-04,EarlyStopping: 1/10test-loss:1.58e-04,Acc:0.8322\n",
      "Epoch 6:train-loss:9.01e-05,val-loss:1.60e-04,EarlyStopping: 2/10test-loss:1.34e-04,Acc:0.8502\n",
      "Epoch 7:train-loss:8.38e-05,val-loss:1.36e-04,test-loss:8.82e-05,Acc:0.8963\n",
      "Epoch 8:train-loss:7.55e-05,val-loss:1.77e-04,EarlyStopping: 1/10test-loss:1.52e-04,Acc:0.8230\n",
      "Epoch 9:train-loss:8.19e-05,val-loss:1.34e-04,test-loss:1.31e-04,Acc:0.8574\n",
      "Epoch 10:train-loss:7.37e-05,val-loss:1.14e-04,test-loss:9.13e-05,Acc:0.8913\n",
      "Epoch 11:train-loss:7.65e-05,val-loss:1.23e-04,EarlyStopping: 1/10test-loss:9.29e-05,Acc:0.8901\n",
      "Epoch 12:train-loss:7.13e-05,val-loss:1.35e-04,EarlyStopping: 2/10test-loss:1.22e-04,Acc:0.8550\n",
      "Epoch 13:train-loss:7.08e-05,val-loss:1.28e-04,EarlyStopping: 3/10test-loss:1.10e-04,Acc:0.8648\n",
      "Epoch 14:train-loss:6.78e-05,val-loss:1.17e-04,EarlyStopping: 4/10test-loss:7.77e-05,Acc:0.9072\n",
      "Epoch 15:train-loss:6.58e-05,val-loss:1.07e-04,test-loss:1.11e-04,Acc:0.8692\n",
      "Epoch 16:train-loss:6.94e-05,val-loss:1.19e-04,EarlyStopping: 1/10test-loss:8.27e-05,Acc:0.9087\n",
      "Epoch 17:train-loss:6.70e-05,val-loss:1.26e-04,EarlyStopping: 2/10test-loss:8.96e-05,Acc:0.8902\n",
      "Epoch 18:train-loss:6.70e-05,val-loss:1.17e-04,EarlyStopping: 3/10test-loss:1.06e-04,Acc:0.8785\n",
      "Epoch 19:train-loss:6.81e-05,val-loss:1.06e-04,test-loss:1.06e-04,Acc:0.8716\n",
      "Epoch 20:train-loss:6.47e-05,val-loss:1.07e-04,EarlyStopping: 1/10test-loss:1.14e-04,Acc:0.8728\n",
      "Epoch 21:train-loss:6.29e-05,val-loss:1.14e-04,EarlyStopping: 2/10test-loss:9.29e-05,Acc:0.8901\n",
      "Epoch 22:train-loss:6.60e-05,val-loss:1.08e-04,EarlyStopping: 3/10test-loss:9.24e-05,Acc:0.8959\n",
      "Epoch 23:train-loss:6.59e-05,val-loss:1.08e-04,EarlyStopping: 4/10test-loss:1.18e-04,Acc:0.8716\n",
      "Epoch 24:train-loss:6.60e-05,val-loss:1.18e-04,EarlyStopping: 5/10test-loss:1.09e-04,Acc:0.8703\n",
      "Epoch 25:train-loss:6.23e-05,val-loss:1.07e-04,EarlyStopping: 6/10test-loss:1.20e-04,Acc:0.8705\n",
      "Epoch 26:train-loss:7.25e-05,val-loss:1.15e-04,EarlyStopping: 7/10test-loss:1.30e-04,Acc:0.8556\n",
      "Epoch 27:train-loss:6.32e-05,val-loss:1.20e-04,EarlyStopping: 8/10test-loss:1.25e-04,Acc:0.8696\n",
      "Epoch 28:train-loss:6.59e-05,val-loss:1.16e-04,EarlyStopping: 9/10test-loss:9.72e-05,Acc:0.8892\n",
      "Epoch 29:train-loss:5.96e-05,val-loss:1.02e-04,test-loss:8.84e-05,Acc:0.9004\n",
      "Epoch 30:train-loss:5.99e-05,val-loss:1.19e-04,EarlyStopping: 1/10test-loss:1.05e-04,Acc:0.8914\n",
      "Epoch 31:train-loss:6.24e-05,val-loss:1.27e-04,EarlyStopping: 2/10test-loss:1.35e-04,Acc:0.8572\n",
      "Epoch 32:train-loss:5.93e-05,val-loss:1.09e-04,EarlyStopping: 3/10test-loss:1.08e-04,Acc:0.8864\n",
      "Epoch 33:train-loss:5.72e-05,val-loss:1.12e-04,EarlyStopping: 4/10test-loss:1.10e-04,Acc:0.8906\n",
      "Epoch 34:train-loss:5.75e-05,val-loss:1.06e-04,EarlyStopping: 5/10test-loss:9.38e-05,Acc:0.8955\n",
      "Epoch 35:train-loss:5.80e-05,val-loss:1.26e-04,EarlyStopping: 6/10test-loss:1.24e-04,Acc:0.8894\n",
      "Epoch 36:train-loss:6.08e-05,val-loss:1.12e-04,EarlyStopping: 7/10test-loss:1.08e-04,Acc:0.8901\n",
      "Epoch 37:train-loss:5.68e-05,val-loss:1.22e-04,EarlyStopping: 8/10test-loss:1.09e-04,Acc:0.8941\n",
      "Epoch 38:train-loss:6.05e-05,val-loss:9.56e-05,test-loss:7.86e-05,Acc:0.9320\n",
      "Epoch 39:train-loss:5.26e-05,val-loss:1.04e-04,EarlyStopping: 1/10test-loss:1.01e-04,Acc:0.9022\n",
      "Epoch 40:train-loss:5.44e-05,val-loss:1.13e-04,EarlyStopping: 2/10test-loss:9.12e-05,Acc:0.9151\n",
      "Epoch 41:train-loss:5.47e-05,val-loss:1.03e-04,EarlyStopping: 3/10test-loss:8.62e-05,Acc:0.9107\n",
      "Epoch 42:train-loss:5.50e-05,val-loss:1.11e-04,EarlyStopping: 4/10test-loss:1.02e-04,Acc:0.8965\n",
      "Epoch 43:train-loss:5.43e-05,val-loss:1.03e-04,EarlyStopping: 5/10test-loss:9.38e-05,Acc:0.9109\n",
      "Epoch 44:train-loss:5.16e-05,val-loss:9.07e-05,test-loss:9.39e-05,Acc:0.9178\n",
      "Epoch 45:train-loss:5.18e-05,val-loss:8.66e-05,test-loss:9.54e-05,Acc:0.9134\n",
      "Epoch 46:train-loss:5.13e-05,val-loss:8.62e-05,test-loss:9.84e-05,Acc:0.9162\n",
      "Epoch 47:train-loss:5.13e-05,val-loss:9.60e-05,EarlyStopping: 1/10test-loss:9.75e-05,Acc:0.9059\n",
      "Epoch 48:train-loss:4.93e-05,val-loss:9.23e-05,EarlyStopping: 2/10test-loss:9.65e-05,Acc:0.9130\n",
      "Epoch 49:train-loss:4.70e-05,val-loss:7.75e-05,test-loss:8.41e-05,Acc:0.9250\n",
      "Epoch 50:train-loss:4.41e-05,val-loss:7.84e-05,EarlyStopping: 1/10test-loss:9.65e-05,Acc:0.9247\n",
      "Final: test-loss:8.60e-05,Acc:0.9248\n",
      "[[0.90642328 0.09357672]\n",
      " [0.02814885 0.97185115]]\n",
      "Epoch 1:train-loss:2.86e-04,val-loss:3.60e-04,test-loss:2.69e-04,Acc:0.6758\n",
      "Epoch 2:train-loss:1.45e-04,val-loss:2.53e-04,test-loss:1.86e-04,Acc:0.8089\n",
      "Epoch 3:train-loss:1.19e-04,val-loss:2.12e-04,test-loss:1.56e-04,Acc:0.8449\n",
      "Epoch 4:train-loss:1.02e-04,val-loss:1.96e-04,test-loss:1.46e-04,Acc:0.8566\n",
      "Epoch 5:train-loss:9.58e-05,val-loss:1.41e-04,test-loss:9.96e-05,Acc:0.8954\n",
      "Epoch 6:train-loss:8.44e-05,val-loss:1.37e-04,test-loss:9.33e-05,Acc:0.9047\n",
      "Epoch 7:train-loss:8.02e-05,val-loss:1.22e-04,test-loss:8.34e-05,Acc:0.9203\n",
      "Epoch 8:train-loss:7.18e-05,val-loss:1.36e-04,EarlyStopping: 1/10test-loss:8.01e-05,Acc:0.9281\n",
      "Epoch 9:train-loss:7.06e-05,val-loss:1.36e-04,EarlyStopping: 2/10test-loss:8.16e-05,Acc:0.9191\n",
      "Epoch 10:train-loss:7.48e-05,val-loss:1.61e-04,EarlyStopping: 3/10test-loss:1.12e-04,Acc:0.8780\n",
      "Epoch 11:train-loss:7.43e-05,val-loss:1.19e-04,test-loss:7.67e-05,Acc:0.9211\n",
      "Epoch 12:train-loss:6.85e-05,val-loss:1.26e-04,EarlyStopping: 1/10test-loss:7.92e-05,Acc:0.9250\n",
      "Epoch 13:train-loss:7.16e-05,val-loss:1.25e-04,EarlyStopping: 2/10test-loss:8.14e-05,Acc:0.9203\n",
      "Epoch 14:train-loss:6.84e-05,val-loss:1.30e-04,EarlyStopping: 3/10test-loss:8.24e-05,Acc:0.9152\n",
      "Epoch 15:train-loss:6.79e-05,val-loss:1.39e-04,EarlyStopping: 4/10test-loss:8.38e-05,Acc:0.9248\n",
      "Epoch 16:train-loss:6.71e-05,val-loss:1.38e-04,EarlyStopping: 5/10test-loss:9.54e-05,Acc:0.8951\n",
      "Epoch 17:train-loss:6.88e-05,val-loss:1.30e-04,EarlyStopping: 6/10test-loss:8.63e-05,Acc:0.9118\n",
      "Epoch 18:train-loss:6.95e-05,val-loss:1.25e-04,EarlyStopping: 7/10test-loss:9.23e-05,Acc:0.9097\n",
      "Epoch 19:train-loss:6.36e-05,val-loss:1.28e-04,EarlyStopping: 8/10test-loss:8.63e-05,Acc:0.9058\n",
      "Epoch 20:train-loss:6.27e-05,val-loss:1.23e-04,EarlyStopping: 9/10test-loss:8.52e-05,Acc:0.9152\n",
      "Epoch 21:train-loss:6.74e-05,val-loss:1.02e-04,test-loss:8.04e-05,Acc:0.9234\n",
      "Epoch 22:train-loss:6.12e-05,val-loss:9.87e-05,test-loss:7.66e-05,Acc:0.9251\n",
      "Epoch 23:train-loss:6.21e-05,val-loss:1.08e-04,EarlyStopping: 1/10test-loss:8.28e-05,Acc:0.9343\n",
      "Epoch 24:train-loss:6.03e-05,val-loss:1.08e-04,EarlyStopping: 2/10test-loss:8.38e-05,Acc:0.9205\n",
      "Epoch 25:train-loss:5.88e-05,val-loss:1.22e-04,EarlyStopping: 3/10test-loss:8.80e-05,Acc:0.9166\n",
      "Epoch 26:train-loss:6.11e-05,val-loss:9.77e-05,test-loss:8.57e-05,Acc:0.9192\n",
      "Epoch 27:train-loss:5.73e-05,val-loss:1.02e-04,EarlyStopping: 1/10test-loss:8.04e-05,Acc:0.9378\n",
      "Epoch 28:train-loss:5.71e-05,val-loss:1.13e-04,EarlyStopping: 2/10test-loss:8.51e-05,Acc:0.9237\n",
      "Epoch 29:train-loss:5.55e-05,val-loss:8.52e-05,test-loss:7.84e-05,Acc:0.9270\n",
      "Epoch 30:train-loss:5.46e-05,val-loss:9.92e-05,EarlyStopping: 1/10test-loss:8.47e-05,Acc:0.9352\n",
      "Epoch 31:train-loss:5.05e-05,val-loss:9.91e-05,EarlyStopping: 2/10test-loss:9.30e-05,Acc:0.9328\n",
      "Epoch 32:train-loss:4.94e-05,val-loss:8.97e-05,EarlyStopping: 3/10test-loss:1.16e-04,Acc:0.9298\n",
      "Epoch 33:train-loss:5.84e-05,val-loss:1.74e-04,EarlyStopping: 4/10test-loss:1.27e-04,Acc:0.8638\n",
      "Epoch 34:train-loss:6.19e-05,val-loss:9.98e-05,EarlyStopping: 5/10test-loss:8.86e-05,Acc:0.9251\n",
      "Epoch 35:train-loss:5.08e-05,val-loss:9.81e-05,EarlyStopping: 6/10test-loss:8.76e-05,Acc:0.9320\n",
      "Epoch 36:train-loss:5.35e-05,val-loss:1.03e-04,EarlyStopping: 7/10test-loss:8.83e-05,Acc:0.9341\n",
      "Epoch 37:train-loss:5.50e-05,val-loss:9.68e-05,EarlyStopping: 8/10test-loss:8.22e-05,Acc:0.9177\n",
      "Epoch 38:train-loss:5.20e-05,val-loss:8.60e-05,EarlyStopping: 9/10test-loss:8.54e-05,Acc:0.9289\n",
      "Epoch 39:train-loss:5.21e-05,val-loss:9.88e-05,EarlyStopping: 10/10Early stopping\n",
      "Final: test-loss:7.89e-05,Acc:0.9271\n",
      "[[0.90858015 0.09141985]\n",
      " [0.02809984 0.97190016]]\n",
      "Epoch 1:train-loss:2.63e-04,val-loss:2.77e-04,test-loss:1.29e-04,Acc:0.8866\n",
      "Epoch 2:train-loss:1.36e-04,val-loss:3.15e-04,EarlyStopping: 1/10test-loss:1.77e-04,Acc:0.8174\n",
      "Epoch 3:train-loss:1.35e-04,val-loss:2.45e-04,test-loss:1.38e-04,Acc:0.8561\n",
      "Epoch 4:train-loss:1.17e-04,val-loss:1.97e-04,test-loss:1.09e-04,Acc:0.8828\n",
      "Epoch 5:train-loss:9.75e-05,val-loss:1.68e-04,test-loss:1.01e-04,Acc:0.8966\n",
      "Epoch 6:train-loss:8.55e-05,val-loss:1.78e-04,EarlyStopping: 1/10test-loss:1.08e-04,Acc:0.8815\n",
      "Epoch 7:train-loss:8.14e-05,val-loss:1.91e-04,EarlyStopping: 2/10test-loss:1.10e-04,Acc:0.8842\n",
      "Epoch 8:train-loss:7.63e-05,val-loss:1.56e-04,test-loss:9.83e-05,Acc:0.8929\n",
      "Epoch 9:train-loss:7.22e-05,val-loss:1.53e-04,test-loss:1.01e-04,Acc:0.8972\n",
      "Epoch 10:train-loss:7.76e-05,val-loss:1.23e-04,test-loss:8.01e-05,Acc:0.9192\n",
      "Epoch 11:train-loss:6.80e-05,val-loss:1.31e-04,EarlyStopping: 1/10test-loss:9.35e-05,Acc:0.9141\n",
      "Epoch 12:train-loss:6.92e-05,val-loss:1.27e-04,EarlyStopping: 2/10test-loss:9.12e-05,Acc:0.9129\n",
      "Epoch 13:train-loss:6.59e-05,val-loss:1.36e-04,EarlyStopping: 3/10test-loss:9.07e-05,Acc:0.9178\n",
      "Epoch 14:train-loss:6.55e-05,val-loss:1.46e-04,EarlyStopping: 4/10test-loss:9.75e-05,Acc:0.9006\n",
      "Epoch 15:train-loss:6.59e-05,val-loss:1.24e-04,EarlyStopping: 5/10test-loss:9.64e-05,Acc:0.9348\n",
      "Epoch 16:train-loss:6.65e-05,val-loss:1.14e-04,test-loss:7.82e-05,Acc:0.9300\n",
      "Epoch 17:train-loss:6.25e-05,val-loss:1.13e-04,test-loss:8.58e-05,Acc:0.9234\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "verbose = True\n",
    "stage = 'onset'\n",
    "plot = False\n",
    "save = False\n",
    "########################################################################################\n",
    "_, _, _, onset_prob = CV_train_onset(CV_datasets=CV_onset_datasets, val_ratio = val_ratio,\n",
    "                                    lr=lr, batch_size=batch_size, patience=patience, \n",
    "                                    class_weight=class_weight, channelNum=256)\n",
    "'''\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(137374, 2)\n"
     ]
    }
   ],
   "source": [
    "np.save(subject+'_opt_onset_prob.npy',onset_prob)###\n",
    "#reload saved data\n",
    "#onset_prob = np.load(subject+'_opt_onset_prob.npy',allow_pickle=True)###\n",
    "print(onset_prob.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [35:18<00:00,  4.24s/trial, best loss: -0.9879744347547571]\n",
      "best {'eps': 4, 'offset_duration': 2, 'onset_duration': 11, 'smooth_threshold': 46, 'smooth_window': 18}\n"
     ]
    }
   ],
   "source": [
    "##############################DO NOT RUN!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "plot = False\n",
    "verbose = False\n",
    "\n",
    "hyperparas = {\n",
    "    #'overt_back':hp.choice('overt_back',[0.1,0.15,0.2,0.25,0.3]),\n",
    "    #'overt_forward':hp.choice('overt_forward', [0.1,0.15,0.2,0.25,0.3]),\n",
    "    #'gruDim':hp.choice('gruDim', [64,128,256,512]),\n",
    "    #'gruLayer':hp.choice('gruLayer', [1,2,3]),\n",
    "    #'drop_out':hp.choice('drop_out', [0.2,0.3,0.4,0.5,0.6,0.7,0.8]),\n",
    "    'smooth_window':hp.choice('smooth_window',np.arange(0.01, 0.05, 0.002).tolist()),\n",
    "    'smooth_threshold':hp.choice('smooth_threshold',np.arange(0.2, 0.9, 0.01).tolist()),\n",
    "    'onset_duration':hp.choice('onset_duration',np.arange(0.02, 0.3, 0.02).tolist()),\n",
    "    'offset_duration':hp.choice('offset_duration',np.arange(0.02, 0.3, 0.02).tolist()),\n",
    "    'eps':hp.choice('eps',np.arange(0.02, 0.2, 0.02).tolist()),\n",
    "    \n",
    "    #'batch_size': hp.choice('batch_size', [512,1024,2048,4096,9192]),\n",
    "    #'lr': hp.choice('lr', [0.001,0.005,0.0005,0.0001]),\n",
    "}\n",
    "\n",
    "trials=Trials()\n",
    "reporterList = []\n",
    "best=fmin(f,hyperparas,algo=tpe.suggest,max_evals=500,trials=trials)\n",
    "verbose=False\n",
    "print('best',best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate clipTimeMat for hyperopt of sylb decoder and tone decoder\n",
    "clipTimeMat, plot_data = slicer(interval, onset_label_mat, predicted=np.vstack(onset_prob)[:,1], sylb_mat = sylb_mat, \n",
    "                       smooth_window = 0.046,\n",
    "                       smooth_threshold = 0.66,\n",
    "                       onset_duration = 0.24,\n",
    "                       offset_duration = 0.06,\n",
    "                       word_duration = 0.5,eps = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(clipTimeMat.shape)\n",
    "np.save(subject+'_opt_clipTimeMat.npy',clipTimeMat)###"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b9ea9ba5ae74b1398a630feb3651b8acb263e18aa2ea7db4a76df02121bff196"
  },
  "kernelspec": {
   "display_name": "Python 3.7.16",
   "language": "python",
   "name": "python3.7.16"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "toc-showcode": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
