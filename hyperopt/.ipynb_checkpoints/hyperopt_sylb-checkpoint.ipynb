{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1118\n"
     ]
    }
   ],
   "source": [
    "#import transformers\n",
    "#from transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from collections import defaultdict\n",
    "from einops import rearrange, repeat\n",
    "from einops.layers.torch import Rearrange\n",
    "\n",
    "import h5py\n",
    "import json\n",
    "\n",
    "import math\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from random import shuffle,randint\n",
    "\n",
    "from scipy.fftpack import fft\n",
    "import scipy.stats as stats\n",
    "import scipy.io as scio\n",
    "from scipy.io import wavfile\n",
    "from scipy import signal\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "from torchmetrics.functional import word_error_rate as wer\n",
    "from textwrap import wrap\n",
    "import textgrid as tg\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from torch import nn, optim, einsum\n",
    "import torch.utils.data as Data\n",
    "from torch.utils.data import DataLoader,TensorDataset\n",
    "import wave\n",
    "\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "#device = 'cpu'\n",
    "date='1118'\n",
    "print(date)\n",
    "subject = 'PA4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nEXAMPLE:\\necog_mat,back_duration,forward_duration,mat = read_ecog_mat(back,forward,mat,channelNum,key_elecs=[],\\n                key_label=[],oppolabel=False,key_sentence=[],opposentence=False, key_paragraph=[],oppoparagraph=False,\\n                hz=False, block=4)\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#FUNCTIONS USED FOR LOAD DATA\n",
    "def get_timelocked_activity(times, hg, back, forward, hz=False):\n",
    "    '''\n",
    "    Get time-locked activity.\n",
    "\n",
    "    Parameters:\n",
    "    - times (array-like): List of timepoints in seconds.\n",
    "    - hg (numpy.ndarray): High gamma array, shaped (elecs_num, whole_duration*hz).\n",
    "    - back (float): Start time before the timepoints.\n",
    "    - forward (float): End time after the timepoints.\n",
    "    - hz (float, optional): Sampling rate of High gamma. Default is False.\n",
    "\n",
    "    Returns:\n",
    "    - Y_mat (numpy.ndarray): Time-locked activity array, shaped (trial_num, elecs_num, selected_duration*hz).\n",
    "    - back (int): Start time in samples (back*hz).\n",
    "    - forward (int): End time in samples (forward*hz).\n",
    "    '''\n",
    "    if hz:\n",
    "        times = (times*hz).astype(int)\n",
    "        back = int(back*hz)\n",
    "        forward = int(forward*hz)\n",
    "    times = times[times - back > 0]\n",
    "    times = times[times + forward < hg.shape[1]]\n",
    "\n",
    "    Y_mat = np.zeros((len(times),hg.shape[0], int(back + forward)), dtype=float)\n",
    "\n",
    "    for i, index in enumerate(times):\n",
    "        Y_mat[i, :, :] = hg[:, int(index-back):int(index+forward)]\n",
    "\n",
    "    return Y_mat,back,forward\n",
    "\n",
    "def read_ecog_mat(back,forward,mat,channelNum,key_elecs=[],key_label=[],oppolabel=False,key_sentence=[],opposentence=False, key_paragraph=[],oppoparagraph=False,\n",
    "                  hz=False, block=4):\n",
    "    '''\n",
    "    get ecog_mat based on defined parameters\n",
    "    \n",
    "    Parameters:\n",
    "    - mat : shaped(7,num_of_timepoints)\n",
    "    - hg (numpy.ndarray): High gamma array, shaped (elecs_num, whole_duration*hz).\n",
    "    - back (float): Start time before the timepoints.\n",
    "    - forward (float): End time after the timepoints.\n",
    "    - hz (float, optional): Sampling rate of High gamma. Default is False.\n",
    "    channelNum:number of electrodes 128 or 256\n",
    "    block:number of blocks included, usually from 1 to 4\n",
    "    key_elecs: selected elecs\n",
    "    kay_labels:selected labels, opplabel means oppose the key_label\n",
    "    sentence and paragraphs similarly\n",
    "    '''\n",
    "\n",
    "    key_elecs = np.array(key_elecs)\n",
    "    key_index = np.ones(len(mat[0,:])).astype('bool')\n",
    "    \n",
    "    sentence_index = np.ones(len(mat[0,:])).astype('bool')\n",
    "    label_index = np.ones(len(mat[0,:])).astype('bool')\n",
    "    paragraph_index = np.ones(len(mat[0,:])).astype('bool')  \n",
    "    \n",
    "    if key_label:\n",
    "        print('select label:'+str(key_label),end=' ')\n",
    "        label_index=~label_index\n",
    "        for i in key_label:\n",
    "            temp_index = mat[0,:] ==i\n",
    "            label_index = np.logical_or(label_index, temp_index)\n",
    "        if oppolabel:\n",
    "            print('oppo')\n",
    "            label_index=~label_index\n",
    "    key_index = np.logical_and(label_index, key_index)\n",
    "    if key_sentence:\n",
    "        print('select sentence:'+str(key_sentence),end=' ')\n",
    "        sentence_index=~sentence_index\n",
    "        for i in key_sentence:\n",
    "            temp_index = mat[2,:] ==i\n",
    "            sentence_index = np.logical_or(sentence_index, temp_index)\n",
    "        if opposentence:\n",
    "            print('oppo')\n",
    "            sentence_index=~sentence_index\n",
    "    key_index = np.logical_and(sentence_index, key_index)        \n",
    "    if key_paragraph:\n",
    "        print('select paraqraph:'+str(key_paragraph),end=' ')\n",
    "        paragraph_index=~paragraph_index\n",
    "        for i in key_paragraph:\n",
    "            temp_index = mat[3,:] ==i\n",
    "            paragraph_index = np.logical_or(paragraph_index, temp_index)\n",
    "        if oppoparagraph:\n",
    "            print('oppo')\n",
    "            paragraph_index=~paragraph_index\n",
    "    key_index = np.logical_and(paragraph_index, key_index)      \n",
    "    mat = mat[:,key_index]\n",
    "       \n",
    "    \n",
    "    if key_elecs.any():\n",
    "        ecog_mat = np.zeros((len(mat[0,:]),len(key_elecs), int((back+forward)*hz)), dtype=float)\n",
    "        print('select elecs:'+str(key_elecs),end=' ')\n",
    "    else:\n",
    "        ecog_mat = np.zeros((len(mat[0,:]),channelNum, int((back+forward)*hz)), dtype=float)\n",
    "        \n",
    "    for i in range(block):\n",
    "        block_index= mat[4,:]==i+1\n",
    "        temp_time_list=mat[1,block_index]\n",
    "        ecogData=scio.loadmat(path_raw+str(i+1)+filterType+'.mat')#################################\n",
    "        ecogData=np.array(ecogData['bands'])*10e4\n",
    "        #print('###',np.nanmean(np.max(ecogData,axis=1)-np.min(ecogData,axis=1)),np.array(np.max(ecogData,axis=1)).shape)######################################################################################\n",
    "        temp_ecog,back_duration,forward_duration = get_timelocked_activity(times=temp_time_list, hg=ecogData, back=back, forward=forward, hz=hz)\n",
    "        if key_elecs.any():\n",
    "            key_elecs = np.array(key_elecs)\n",
    "            ecog_mat[block_index] = temp_ecog[:,key_elecs,:]\n",
    "        else:\n",
    "            ecog_mat[block_index] = temp_ecog\n",
    "\n",
    "    print('read_ecog:',ecog_mat.shape)\n",
    "    return ecog_mat,back_duration,forward_duration,mat\n",
    "\n",
    "'''\n",
    "EXAMPLE:\n",
    "ecog_mat,back_duration,forward_duration,mat = read_ecog_mat(back,forward,mat,channelNum,key_elecs=[],\n",
    "                key_label=[],oppolabel=False,key_sentence=[],opposentence=False, key_paragraph=[],oppoparagraph=False,\n",
    "                hz=False, block=4)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FUNCTIONS USED FOR TRAIN AND VALIDATE THE MODEL\n",
    "\n",
    "def evaluater(Mat, prob_list, loss_func = nn.CrossEntropyLoss(), unit = 3):\n",
    "    \n",
    "    if stage in ['sylb','onset_sylb']:\n",
    "        List = sylbList\n",
    "        row = 0\n",
    "    elif stage in ['tone','onset_tone']:\n",
    "        List = toneList\n",
    "        row = 5\n",
    "        \n",
    "    real_indices = np.where(~np.isnan(Mat[0, :]))[0]\n",
    "    Mat =  Mat[:, real_indices]\n",
    "    prob_list = np.array(prob_list)[np.array(real_indices)]\n",
    "    \n",
    "    acc_list=[]\n",
    "    loss_list=[]\n",
    "    for trial in set(Mat[unit,:]):\n",
    "        trial_index = Mat[3,:]==trial\n",
    "        temp_label = Mat[row,trial_index]\n",
    "        temp_prob = prob_list[trial_index]\n",
    "\n",
    "        loss = loss_func(torch.tensor(temp_prob), torch.tensor(temp_label, dtype=torch.long))\n",
    "        loss = loss / len(temp_label)\n",
    "        temp_out = np.argmax(temp_prob, axis=1)\n",
    "        correct = np.sum(temp_out == np.array(temp_label))\n",
    "        acc = correct / len(temp_label)\n",
    "\n",
    "        acc_list.append(acc)\n",
    "        loss_list.append(loss)\n",
    "    \n",
    "    if plot:\n",
    "        plotCM(List, acc_list, Mat[row,:], np.argmax(prob_list, axis=1))\n",
    "\n",
    "    return acc_list, loss_list\n",
    "\n",
    "def CV_datasets(back,forward,mat,key_elecs,row,CV_list,unbalance=False,List=False,augmented=False):\n",
    "    #split data to cross validation combinations, and calculate the weight of each labels\n",
    "    CV_datasets = []\n",
    "    CV_augmented_datasets=[]\n",
    "    count_total=[]\n",
    "\n",
    "    for CVs in range(6):\n",
    "        print('mat:',mat.shape)\n",
    "        test_ecog_mat,_,_,test_mat= read_ecog_mat(back=back,forward=forward,mat=mat,channelNum=channelNum,key_elecs=key_elecs,\n",
    "                key_label=[],oppolabel=False,key_sentence=[],opposentence=False, key_paragraph=CV_list[CVs],oppoparagraph=False,\n",
    "                hz=hz, block=4)\n",
    "\n",
    "        test_label = test_mat[row,:]\n",
    "        \n",
    "        if unbalance == True:\n",
    "            count=np.bincount(test_label.astype('int'))\n",
    "            if len(count)!=len(List):\n",
    "                #triggerrd when there are some blocks without all the labels\n",
    "                print('not a full set!')\n",
    "            print(count)\n",
    "            count_total.append(count)\n",
    "        test_ecog_mat=torch.FloatTensor(test_ecog_mat)\n",
    "        test_label=torch.tensor(test_label,dtype=torch.long)\n",
    "        testdataset = Data.TensorDataset(test_ecog_mat, test_label)    \n",
    "        CV_datasets.append(testdataset)\n",
    "        \n",
    "\n",
    "        if augmented is not False:\n",
    "            aug_mat = stammer (mat=test_mat, multiple_time=augmented,multiple_range=0.1,shift=-0.05)\n",
    "            aug_ecog_mat,_,_,test_mat= read_ecog_mat(back=back,forward=forward,mat=mat,channelNum=channelNum,key_elecs=key_elecs,\n",
    "                key_label=[],oppolabel=False,key_sentence=[],opposentence=False, key_paragraph=CV_list[CVs],oppoparagraph=False,\n",
    "                hz=hz, block=4)\n",
    "            aug_label = aug_mat[row,:]\n",
    "            aug_ecog_mat=torch.FloatTensor(aug_ecog_mat)\n",
    "            aug_label=torch.tensor(aug_label,dtype=torch.long)\n",
    "            augdataset = Data.TensorDataset(aug_ecog_mat, aug_label) \n",
    "            CV_augmented_datasets.append(augdataset)\n",
    "            \n",
    "    if unbalance == True:\n",
    "        count_total = np.array(count_total)\n",
    "        \n",
    "    return CV_datasets,count_total,CV_augmented_datasets\n",
    "\n",
    "'''\n",
    "EXAMPLES:\n",
    "CV_onset_datasets,count_total,_ = CV_datasets(back=overt_back,\n",
    "                                            forward=overt_forward,\n",
    "                                            mat=overt_mat,\n",
    "                                            key_elecs=overt_elecs,\n",
    "                                            row=0,\n",
    "                                            CV_list,\n",
    "                                            unbalance=True,List=stateList,\n",
    "                                            augmented=False)\n",
    "class_weight = np.sum(count_total,axis=0)\n",
    "'''\n",
    "'''\n",
    "CV_sylb_datasets,_,_ = CV_datasets(back=sylb_back,\n",
    "                            forward=sylb_forward,\n",
    "                            mat=sylb_mat,\n",
    "                            key_elecs=sylb_elecs,\n",
    "                            row=0,\n",
    "                            CV_list,\n",
    "                            unbalance=False,List=False,\n",
    "                            augmented=False)\n",
    "'''\n",
    "'''\n",
    "CV_clip_datasets,_,_ = CV_datasets(back=sylb_back,\n",
    "                            forward=sylb_forward,\n",
    "                            mat=clipTimeMat,\n",
    "                            key_elecs=sylb_elecs,\n",
    "                            row=0,\n",
    "                            CV_list,\n",
    "                            unbalance=False,List=False,\n",
    "                            augmented=False)\n",
    "\n",
    "'''\n",
    "def CV_train_ENN(CV_datasets, CV_pred_only_datasets, lr, batch_size, patience,\n",
    "                 class_weight = False, channelNum=256,plot = False):\n",
    "    #this is used for cross validation on ensumble models \n",
    "    test_out_prob_list=[]\n",
    "    pred_only_prob_list=[]\n",
    "    torch.cuda.empty_cache()\n",
    "    for test_CV in range(6):#tqdm(\n",
    "        CV_train_val_datasets = CV_datasets.copy()\n",
    "        CV_test_datasets = [CV_train_val_datasets.pop(test_CV),]\n",
    "        pred_only_datasets = [CV_pred_only_datasets[test_CV],]\n",
    "        #model_list=[]\n",
    "        predicted_prob_list = []\n",
    "        predicted_only_prob_list= []\n",
    "        for CVs in range(5):\n",
    "            CV_train_datasets = CV_train_val_datasets.copy()\n",
    "            CV_val_datasets = [CV_train_datasets.pop(CVs),]\n",
    "            #label,acc,predicted,predicted_prob, predicted_only,predicted_only_prob,model\n",
    "            _,_,_,predicted_prob,_,predicted_only_prob,CV_model,loss_func = train( \n",
    "                                        lr=lr, batch_size=batch_size, EPOCH=EPOCH, patience=patience, \n",
    "                                        CV_train_datasets = CV_train_datasets, #CV_train_val_datasets, #\n",
    "                                        CV_val_datasets =  CV_val_datasets, #False, #\n",
    "                                        CV_test_datasets = CV_test_datasets,#False, \n",
    "                                        pred_only_datasets = pred_only_datasets, \n",
    "                                        class_weight=class_weight)\n",
    "            \n",
    "            #model_list.append(CV_model)\n",
    "            predicted_prob_list.append(predicted_prob)\n",
    "            predicted_only_prob_list.append(predicted_only_prob)\n",
    "        \n",
    "        predicted_prob = np.mean(predicted_prob_list, axis=0)\n",
    "        test_out_prob_list.extend(predicted_prob)\n",
    "        predicted_only_prob = np.mean(predicted_only_prob_list, axis=0)\n",
    "        pred_only_prob_list.extend(predicted_only_prob) \n",
    "        \n",
    "    a, acc_list = evaluater(sylb_label_mat, test_out_prob_list, loss_func = loss_func, unit = 3)    \n",
    "    if not verbose:\n",
    "        print(str(a))\n",
    "    _, loss_list = evaluater(clipTimeMat, pred_only_prob_list, loss_func = loss_func, unit = 3)\n",
    "    \n",
    "    return [acc_list, loss_list], test_out_prob_list, pred_only_prob_list\n",
    "\n",
    "def train(batch_size, lr, EPOCH, patience, CV_train_datasets, \n",
    "          CV_val_datasets=False, CV_test_datasets=False, pred_only_datasets=False, \n",
    "          class_weight=False):\n",
    "    #This function directly train and validate the model\n",
    "    model = torch.load(\"./\"+subject+'.pt')\n",
    "    loss_func = nn.CrossEntropyLoss() \n",
    "    if class_weight is not False:\n",
    "        weight = [max(class_weight)/x for x in class_weight]\n",
    "        weight = torch.FloatTensor(weight).to(device)\n",
    "        loss_func = nn.CrossEntropyLoss(weight=weight)\n",
    "        \n",
    "    optimizer = torch.optim.Adam(model.parameters(),lr=lr)#也可以使用SGD优化算法进行训练\n",
    "    \n",
    "    early_stopping = EarlyStopping(patience)\n",
    "    '''\n",
    "    optimizer = torch.optim.SGD(model.parameters(),lr=lr)#也可以使用SGD优化算法进行训练\n",
    "    scheduler =  torch.optim.lr_scheduler.CosineAnnealingLR(optimizer = optimizer,T_max =  EPOCH)\n",
    "    \n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=patience, \n",
    "                                           verbose=False, threshold=0.0001, threshold_mode='rel', \n",
    "                                           cooldown=0, min_lr=0, eps=1e-08)\n",
    "    '''\n",
    "    for epoch in range(EPOCH):\n",
    "        #return acc，ground truth & predicited label\n",
    "        sum_loss_train= 0\n",
    "        total_train=0\n",
    "        model.train()\n",
    "        for train_dataset in CV_train_datasets:\n",
    "            train_loader = Data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "            for data in train_loader:\n",
    "                inputs, labels = data\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                pred = model(inputs)\n",
    "                loss = loss_func(pred, labels)\n",
    "                sum_loss_train+= loss.item()\n",
    "                total_train+= labels.size(0)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "        train_loss = sum_loss_train/total_train\n",
    "        if verbose:\n",
    "            print('Epoch {}:train-loss:{:.2e},'.format(epoch+1,train_loss),end='')\n",
    "       \n",
    "        if CV_val_datasets is not False:#if there is val datasets performing early-stopping\n",
    "            sum_loss_val=0\n",
    "            total_val = 0  \n",
    "            for val_dataset in CV_val_datasets:\n",
    "                val_loader = Data.DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "                for data in val_loader:\n",
    "                    inputs, labels = data\n",
    "                    inputs, labels = inputs.to(device), labels.to(device) \n",
    "                    pred = model(inputs)\n",
    "                    loss = loss_func(pred, labels)\n",
    "                    sum_loss_val+=loss.item()\n",
    "                    total_val+= labels.size(0)\n",
    "            val_loss = sum_loss_val/total_val\n",
    "            if verbose:\n",
    "                print('val-loss:{:.2e},'.format(val_loss,),end='')\n",
    "            \n",
    "            '''\n",
    "            scheduler.step(val_loss)##############################################\n",
    "            #scheduler.step()\n",
    "            '''\n",
    "            early_stopping(val_loss, model)\n",
    "            if early_stopping.early_stop:\n",
    "                if verbose:\n",
    "                    print(\"Early stopping\")\n",
    "                break\n",
    "        \n",
    "        if CV_test_datasets is not False: #if there is test data for testing\n",
    "            total_test = 0  \n",
    "            sum_loss_test= 0\n",
    "            predicted=[]\n",
    "            label=[]\n",
    "            \n",
    "            for test_dataset in CV_test_datasets:\n",
    "                test_loader = Data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "                for data in test_loader:\n",
    "                    inputs, labels = data\n",
    "                    inputs, labels = inputs.to(device), labels.to(device)\n",
    "                    pred_prob = model(inputs)\n",
    "                    loss = loss_func(pred_prob, labels)\n",
    "                    sum_loss_test+=loss.item()\n",
    "                    _, pred = torch.max(pred_prob.data, 1)\n",
    "                    pred_prob = pred_prob.data\n",
    "                    pred_prob = F.softmax(pred_prob, dim=1) \n",
    "                    total_test+= labels.size(0)\n",
    "                    predicted.append(pred.cpu())\n",
    "                    label.append(labels.cpu())\n",
    "            label = torch.cat(label,dim=0)\n",
    "            predicted = torch.cat(predicted,dim=0)\n",
    "            correct = (predicted == label).sum()\n",
    "            test_acc= correct.item()/total_test\n",
    "            test_loss=sum_loss_test/total_test\n",
    "            if verbose:\n",
    "                print('test-loss:{:.2e},Acc:{:.4f}'.format(test_loss,test_acc),end='')\n",
    "        if verbose:\n",
    "            print('')\n",
    "    \n",
    "    model.load_state_dict(torch.load('checkpoint.pt'))\n",
    "    \n",
    "    ##############################EVALUATION model###############################################\n",
    "    predicted=[]\n",
    "    predicted_prob=[]\n",
    "    label=[]\n",
    "    acc=[]\n",
    "    if CV_test_datasets is not False:\n",
    "        total_test = 0  # 总数\n",
    "        sum_loss_test= 0\n",
    "        model.eval()\n",
    "        for test_dataset in CV_test_datasets:\n",
    "            test_loader = Data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "            for data in test_loader:\n",
    "                inputs, labels = data\n",
    "                inputs, labels = inputs.to(device), labels.to(device)  # 有GPU则将数据置入GPU加速\n",
    "                pred_prob = model(inputs)\n",
    "                loss = loss_func(pred_prob, labels)\n",
    "                sum_loss_test+=loss.item()\n",
    "                _, pred = torch.max(pred_prob.data, 1)\n",
    "                pred_prob = pred_prob.data\n",
    "                pred_prob = F.softmax(pred_prob, dim=1) \n",
    "                total_test+= labels.size(0)\n",
    "\n",
    "                predicted.append(pred.cpu())\n",
    "                predicted_prob.append(pred_prob.cpu())\n",
    "                label.append(labels.cpu())\n",
    "\n",
    "        label = torch.cat(label,dim=0)\n",
    "        predicted = torch.cat(predicted,dim=0)\n",
    "        predicted_prob = torch.cat(predicted_prob,dim=0)\n",
    "        correct = (predicted == label).sum()\n",
    "        test_acc= correct.item()/total_test\n",
    "        test_loss=sum_loss_test/total_test\n",
    "        if verbose:\n",
    "            print('Final: test-loss:{:.2e},Acc:{:.4f}'.format(test_loss,test_acc))\n",
    "        label = label.detach().numpy()\n",
    "        acc = test_acc\n",
    "        predicted = predicted.detach().numpy()\n",
    "        predicted_prob = predicted_prob.detach().numpy()\n",
    "    \n",
    "    \n",
    "    ##PRED-ONLY:do not return acc, only predicted result\n",
    "    predicted_only=[]\n",
    "    predicted_only_prob=[]\n",
    "        \n",
    "    if pred_only_datasets is not False:\n",
    "        for pred_only_dataset in pred_only_datasets:\n",
    "            pred_only_loader = Data.DataLoader(pred_only_dataset, batch_size=batch_size, shuffle=False)\n",
    "            for inputs in pred_only_loader:\n",
    "                inputs = inputs[0].to(device)\n",
    "                pred = model(inputs)\n",
    "                pred_only_prob = pred.data\n",
    "                pred_only_prob = F.softmax(pred_only_prob, dim=1) \n",
    "                _, pred_only = torch.max(pred.data, 1)\n",
    "                predicted_only.append(pred_only.cpu())\n",
    "                predicted_only_prob.append(pred_only_prob.cpu())\n",
    "            \n",
    "        predicted_only = torch.cat(predicted_only).detach().numpy()\n",
    "        predicted_only_prob = torch.cat(predicted_only_prob).detach().numpy()\n",
    "        if verbose:\n",
    "            print(predicted_only_prob.shape)\n",
    "    \n",
    "    return label,acc, predicted,predicted_prob, predicted_only,predicted_only_prob, model,loss_func.cpu()\n",
    "\n",
    "class EarlyStopping:\n",
    "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
    "    def __init__(self, patience=7, delta=0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): How long to wait after last time validation loss improved.\n",
    "                            Default: 7\n",
    "            verbose (bool): If True, prints a message for each validation loss improvement. \n",
    "                            Default: False\n",
    "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
    "                            Default: 0\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.delta = delta\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                print(f'EarlyStopping: {self.counter}/{self.patience}',end='')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        '''Saves model when validation loss decrease.'''\n",
    "        torch.save(model.state_dict(), 'checkpoint.pt')\t# save the best model\n",
    "        self.val_loss_min = val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7, 1034)\n",
      "select paraqraph:[13, 14, 15, 16] oppo\n",
      "select elecs:[1] read_ecog: (775, 1, 8)\n",
      "(7, 768)\n"
     ]
    }
   ],
   "source": [
    "path_raw='../Raw/B'\n",
    "\n",
    "sylbList = ['shi', 'de', 'ji', 'li', 'bu', 'ge', 'qi', 'zhe', 'ta', 'zhi']\n",
    "stateList=['silent','speech']\n",
    "toneList = ['1','2','3','4']\n",
    "hz = 400\n",
    "filterType = '_70_150'\n",
    "channelNum = 256\n",
    "sylb_mat = np.load('../'+subject+'_sylb_mat'+'.npy',allow_pickle=True)\n",
    "print(sylb_mat.shape)\n",
    "_,_,_,sylb_label_mat =  read_ecog_mat(0.01,0.01,sylb_mat,channelNum=256,key_elecs=[1,],\n",
    "                key_label=[],oppolabel=False,key_sentence=[],opposentence=False, \n",
    "                key_paragraph=[13,14,15,16],oppoparagraph=True,hz=hz, block=4)\n",
    "clipTimeMat = np.load(subject+'_opt_clipTimeMat'+'.npy',allow_pickle=True)\n",
    "print(clipTimeMat.shape)\n",
    "\n",
    "channelNum = 256\n",
    "sylb_back = 0.4\n",
    "sylb_forward = 0.8\n",
    "\n",
    "CV_list=[[1,2],[3,4],[5,6],[7,8],[9,10],[11,12]]\n",
    "sylb_elecs = np.load('../'+subject+'_anovasylb_elecs.npy')\n",
    "row=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\na = torch.zeros([10,32,480])\\nprint(a.shape)\\nmodel = timespatCNNRNN(duration=480, typeNum=10, in_chans=32, \\n                 n_filters_time=64,\\n                 filter_time_length=2,\\n                 n_filters_spat=64,\\n                 conv_stride=2,\\n                 pool_time_length=2,\\n                 pool_stride=2,\\n                 gruDim=64)\\n                        \\n#model = CNNRNN(duration = 480, typeNum = 10, channelNum=256, kernel=2, stride=2, mp_kernel=2, gruDim=64)\\nb = model(a)\\nprint(b)\\n\\nfor name, param in model.named_parameters():\\n    print('Parameter name:', name)\\n    print('Parameter shape:',param.shape)\\n\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class timespatCNNRNN(nn.Module):\n",
    "    def __init__(self, *, duration, typeNum, in_chans, is_timespat,\n",
    "                 n_filters_time,\n",
    "                 filter_time_length,\n",
    "                 n_filters_spat,\n",
    "                 conv_stride,\n",
    "                 pool_time_length,\n",
    "                 pool_stride,\n",
    "                 n_filters,\n",
    "                 filter_length, \n",
    "                 n_CNN_layer,\n",
    "                 gruDim,\n",
    "                 gruLayer,\n",
    "                 drop_out):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv_time = nn.Conv2d(\n",
    "            1,\n",
    "            n_filters_time,\n",
    "            (filter_time_length, 1),\n",
    "            stride=1,\n",
    "        )\n",
    "        \n",
    "        self.conv_spat = nn.Conv2d(\n",
    "            n_filters_time,\n",
    "            n_filters_spat,\n",
    "            (1, in_chans),\n",
    "            stride=(conv_stride, 1),\n",
    "        )\n",
    "        self.is_timespat = is_timespat\n",
    "        self.conv_timespat = nn.Conv2d(\n",
    "            1,\n",
    "            n_filters_spat,\n",
    "            (filter_time_length, in_chans),\n",
    "            stride=(conv_stride, 1),\n",
    "        )\n",
    "        \n",
    "        self.bnorm = nn.BatchNorm2d(\n",
    "            n_filters_spat,\n",
    "            #momentum=self.batch_norm_alpha,\n",
    "            affine=True,\n",
    "            eps=1e-5)\n",
    "        self.elu = nn.ELU()\n",
    "        self.pool = nn.MaxPool2d(kernel_size=(pool_time_length, 1), stride=(pool_stride, 1))\n",
    "        \n",
    "        self.conv_pool_block = nn.ModuleList()\n",
    "        self.conv_pool_block.append(nn.Dropout(p=drop_out))\n",
    "        self.conv_pool_block.append(nn.Conv2d(\n",
    "            n_filters_spat,\n",
    "            n_filters,\n",
    "            (filter_length, 1),\n",
    "            stride=(conv_stride, 1),\n",
    "            padding=(((filter_length - 1) * conv_stride) // 2,0)\n",
    "        ))\n",
    "        \n",
    "        for i in range(n_CNN_layer-1):\n",
    "            self.conv_pool_block.append(nn.Dropout(p=drop_out))\n",
    "            self.conv_pool_block.append(nn.Conv2d(\n",
    "                n_filters,\n",
    "                n_filters,\n",
    "                (filter_length, 1),\n",
    "                stride=(conv_stride, 1),\n",
    "                padding=(((filter_length - 1) * conv_stride) // 2,0)\n",
    "            ))\n",
    "            self.conv_pool_block.append(nn.BatchNorm2d(\n",
    "                n_filters,\n",
    "                momentum=0.1,\n",
    "                affine=True,\n",
    "                eps=1e-5,\n",
    "            ))\n",
    "            self.conv_pool_block.append(nn.ELU())\n",
    "            self.conv_pool_block.append(nn.MaxPool2d(\n",
    "                kernel_size=(pool_time_length, 1),\n",
    "                stride=(pool_stride, 1),\n",
    "            ))\n",
    "\n",
    "        self.gru1 = nn.GRU(n_filters, gruDim, gruLayer, batch_first=True, bidirectional=True)\n",
    "        elec_feature = int(2*gruDim)\n",
    "        self.fc1 = nn.Linear(elec_feature, typeNum)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = rearrange(x,'(batch 1) electrodes duration -> batch 1 duration electrodes')\n",
    "        if self.is_timespat:\n",
    "            x = self.conv_timespat(x)\n",
    "        else:\n",
    "            x = self.conv_time(x)\n",
    "            #print('1',x.shape)\n",
    "            x = self.conv_spat(x)\n",
    "        #print('2',x.shape)\n",
    "        x = self.bnorm(x)\n",
    "        x = self.elu(x)\n",
    "        x = self.pool(x)\n",
    "        for block in self.conv_pool_block:\n",
    "            x = block(x)\n",
    "        x = rearrange(x,'batch filter duration 1 -> batch duration filter')\n",
    "        x = self.gru1(x)[0][:,-1,:]\n",
    "        #x = self.relu1(x) \n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    "\n",
    "####################################################################################################\n",
    "'''\n",
    "a = torch.zeros([10,32,480])\n",
    "print(a.shape)\n",
    "model = timespatCNNRNN(duration=480, typeNum=10, in_chans=32, \n",
    "                 n_filters_time=64,\n",
    "                 filter_time_length=2,\n",
    "                 n_filters_spat=64,\n",
    "                 conv_stride=2,\n",
    "                 pool_time_length=2,\n",
    "                 pool_stride=2,\n",
    "                 gruDim=64)\n",
    "                        \n",
    "#model = CNNRNN(duration = 480, typeNum = 10, channelNum=256, kernel=2, stride=2, mp_kernel=2, gruDim=64)\n",
    "b = model(a)\n",
    "print(b)\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    print('Parameter name:', name)\n",
    "    print('Parameter shape:',param.shape)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mat: (7, 1034)\n",
      "select paraqraph:[1, 2] select elecs:[  0   1   2   5   8   9  13  14  15  32  46  48  49  59  64  95  96 100\n",
      " 110 111 112 126 128 135 136 137 138 139 140 142 143 150 151 152 153 154\n",
      " 155 156 157 158 159 166 167 168 169 170 171 172 173 174 175 180 181 182\n",
      " 183 184 185 186 187 188 189 190 191 197 199 201 204 214 215 226 231 242\n",
      " 243 253 254] read_ecog: (133, 75, 480)\n",
      "mat: (7, 1034)\n",
      "select paraqraph:[3, 4] select elecs:[  0   1   2   5   8   9  13  14  15  32  46  48  49  59  64  95  96 100\n",
      " 110 111 112 126 128 135 136 137 138 139 140 142 143 150 151 152 153 154\n",
      " 155 156 157 158 159 166 167 168 169 170 171 172 173 174 175 180 181 182\n",
      " 183 184 185 186 187 188 189 190 191 197 199 201 204 214 215 226 231 242\n",
      " 243 253 254] read_ecog: (132, 75, 480)\n",
      "mat: (7, 1034)\n",
      "select paraqraph:[5, 6] select elecs:[  0   1   2   5   8   9  13  14  15  32  46  48  49  59  64  95  96 100\n",
      " 110 111 112 126 128 135 136 137 138 139 140 142 143 150 151 152 153 154\n",
      " 155 156 157 158 159 166 167 168 169 170 171 172 173 174 175 180 181 182\n",
      " 183 184 185 186 187 188 189 190 191 197 199 201 204 214 215 226 231 242\n",
      " 243 253 254] read_ecog: (125, 75, 480)\n",
      "mat: (7, 1034)\n",
      "select paraqraph:[7, 8] select elecs:[  0   1   2   5   8   9  13  14  15  32  46  48  49  59  64  95  96 100\n",
      " 110 111 112 126 128 135 136 137 138 139 140 142 143 150 151 152 153 154\n",
      " 155 156 157 158 159 166 167 168 169 170 171 172 173 174 175 180 181 182\n",
      " 183 184 185 186 187 188 189 190 191 197 199 201 204 214 215 226 231 242\n",
      " 243 253 254] read_ecog: (129, 75, 480)\n",
      "mat: (7, 1034)\n",
      "select paraqraph:[9, 10] select elecs:[  0   1   2   5   8   9  13  14  15  32  46  48  49  59  64  95  96 100\n",
      " 110 111 112 126 128 135 136 137 138 139 140 142 143 150 151 152 153 154\n",
      " 155 156 157 158 159 166 167 168 169 170 171 172 173 174 175 180 181 182\n",
      " 183 184 185 186 187 188 189 190 191 197 199 201 204 214 215 226 231 242\n",
      " 243 253 254] read_ecog: (124, 75, 480)\n",
      "mat: (7, 1034)\n",
      "select paraqraph:[11, 12] select elecs:[  0   1   2   5   8   9  13  14  15  32  46  48  49  59  64  95  96 100\n",
      " 110 111 112 126 128 135 136 137 138 139 140 142 143 150 151 152 153 154\n",
      " 155 156 157 158 159 166 167 168 169 170 171 172 173 174 175 180 181 182\n",
      " 183 184 185 186 187 188 189 190 191 197 199 201 204 214 215 226 231 242\n",
      " 243 253 254] read_ecog: (132, 75, 480)\n"
     ]
    }
   ],
   "source": [
    "CV_list=[[1,2],[3,4],[5,6],[7,8],[9,10],[11,12]]\n",
    "#load datasets of syllable decoding on manually aligned onsets\n",
    "CV_sylb_datasets,_,_ = CV_datasets(back=sylb_back,\n",
    "                            forward=sylb_forward,\n",
    "                            mat=sylb_mat,\n",
    "                            key_elecs=sylb_elecs,\n",
    "                            row=0,\n",
    "                            CV_list=CV_list,\n",
    "                            unbalance=False,List=False,\n",
    "                            augmented=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mat: (7, 768)\n",
      "select paraqraph:[1, 2] select elecs:[  0   1   2   5   8   9  13  14  15  32  46  48  49  59  64  95  96 100\n",
      " 110 111 112 126 128 135 136 137 138 139 140 142 143 150 151 152 153 154\n",
      " 155 156 157 158 159 166 167 168 169 170 171 172 173 174 175 180 181 182\n",
      " 183 184 185 186 187 188 189 190 191 197 199 201 204 214 215 226 231 242\n",
      " 243 253 254] read_ecog: (132, 75, 480)\n",
      "mat: (7, 768)\n",
      "select paraqraph:[3, 4] select elecs:[  0   1   2   5   8   9  13  14  15  32  46  48  49  59  64  95  96 100\n",
      " 110 111 112 126 128 135 136 137 138 139 140 142 143 150 151 152 153 154\n",
      " 155 156 157 158 159 166 167 168 169 170 171 172 173 174 175 180 181 182\n",
      " 183 184 185 186 187 188 189 190 191 197 199 201 204 214 215 226 231 242\n",
      " 243 253 254] read_ecog: (130, 75, 480)\n",
      "mat: (7, 768)\n",
      "select paraqraph:[5, 6] select elecs:[  0   1   2   5   8   9  13  14  15  32  46  48  49  59  64  95  96 100\n",
      " 110 111 112 126 128 135 136 137 138 139 140 142 143 150 151 152 153 154\n",
      " 155 156 157 158 159 166 167 168 169 170 171 172 173 174 175 180 181 182\n",
      " 183 184 185 186 187 188 189 190 191 197 199 201 204 214 215 226 231 242\n",
      " 243 253 254] read_ecog: (126, 75, 480)\n",
      "mat: (7, 768)\n",
      "select paraqraph:[7, 8] select elecs:[  0   1   2   5   8   9  13  14  15  32  46  48  49  59  64  95  96 100\n",
      " 110 111 112 126 128 135 136 137 138 139 140 142 143 150 151 152 153 154\n",
      " 155 156 157 158 159 166 167 168 169 170 171 172 173 174 175 180 181 182\n",
      " 183 184 185 186 187 188 189 190 191 197 199 201 204 214 215 226 231 242\n",
      " 243 253 254] read_ecog: (127, 75, 480)\n",
      "mat: (7, 768)\n",
      "select paraqraph:[9, 10] select elecs:[  0   1   2   5   8   9  13  14  15  32  46  48  49  59  64  95  96 100\n",
      " 110 111 112 126 128 135 136 137 138 139 140 142 143 150 151 152 153 154\n",
      " 155 156 157 158 159 166 167 168 169 170 171 172 173 174 175 180 181 182\n",
      " 183 184 185 186 187 188 189 190 191 197 199 201 204 214 215 226 231 242\n",
      " 243 253 254] read_ecog: (123, 75, 480)\n",
      "mat: (7, 768)\n",
      "select paraqraph:[11, 12] select elecs:[  0   1   2   5   8   9  13  14  15  32  46  48  49  59  64  95  96 100\n",
      " 110 111 112 126 128 135 136 137 138 139 140 142 143 150 151 152 153 154\n",
      " 155 156 157 158 159 166 167 168 169 170 171 172 173 174 175 180 181 182\n",
      " 183 184 185 186 187 188 189 190 191 197 199 201 204 214 215 226 231 242\n",
      " 243 253 254] read_ecog: (130, 75, 480)\n"
     ]
    }
   ],
   "source": [
    "CV_list=[[1,2],[3,4],[5,6],[7,8],[9,10],[11,12]]\n",
    "#load datasets of syllable decoding on the onsets detected by optimized speech detector\n",
    "CV_clip_datasets,_,_ = CV_datasets(back=sylb_back,\n",
    "                            forward=sylb_forward,\n",
    "                            mat=clipTimeMat,\n",
    "                            key_elecs=sylb_elecs,\n",
    "                            row=0,\n",
    "                            CV_list=CV_list,\n",
    "                            unbalance=False,List=False,\n",
    "                            augmented=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5671641791044776, 0.4696969696969697, 0.3787878787878788, 0.4393939393939394, 0.4090909090909091, 0.3728813559322034, 0.6212121212121212, 0.6507936507936508, 0.5423728813559322, 0.5076923076923077, 0.4090909090909091, 0.4090909090909091]\n",
      "  0%|          | 1/500 [20:44<172:29:11, 1244.39s/trial, best loss: 0.06284551322460175]"
     ]
    }
   ],
   "source": [
    "EPOCH=1000\n",
    "patience2=50\n",
    "\n",
    "stage='sylb'\n",
    "verbose = False\n",
    "plot = False\n",
    "\n",
    "reporterList=[]\n",
    "def hyperopt_syllable(filter_time_length,n_filters,conv_stride,pool_time_length,pool_stride,filter_length,\n",
    "                      n_CNN_layer,gruDim,gruLayer,drop_out):    \n",
    "    batch_size_sylb = 512\n",
    "    lr_sylb = 0.0005#lr\n",
    "    model2 = timespatCNNRNN(duration=int((sylb_back+sylb_forward)*hz), typeNum=10, in_chans=len(sylb_elecs),\n",
    "                                    is_timespat=True,\n",
    "                                    n_filters_time=n_filters, filter_time_length=filter_time_length,\n",
    "                                    n_filters_spat=n_filters, conv_stride=conv_stride,\n",
    "                                    pool_time_length=pool_time_length, pool_stride=pool_stride,\n",
    "                                    n_filters=n_filters, \n",
    "                                    filter_length=filter_length, \n",
    "                                    n_CNN_layer=n_CNN_layer,\n",
    "                                    gruDim=gruDim,\n",
    "                                    gruLayer=gruLayer,\n",
    "                                    drop_out=drop_out).to(device)\n",
    "    \n",
    "    torch.save(model2,(\"./\"+subject+'_sylb.pt'))\n",
    "    torch.save(model2,(\"./\"+subject+'.pt'))\n",
    "    lists, _, _ = CV_train_ENN(CV_datasets = CV_sylb_datasets, \n",
    "                                   CV_pred_only_datasets = CV_clip_datasets,\n",
    "                                   lr=lr_sylb, batch_size=batch_size_sylb, patience=patience2,\n",
    "                                   class_weight = False, channelNum=256)\n",
    "    \n",
    "    #a = np.array([np.nanmean(loss_list),is_timespat,batch_size,lr,conv_stride,pool_time_length,pool_stride,filter_length])\n",
    "    a = np.array([np.nanmean(lists[1])+np.nanmean(lists[0]),np.nanmean(lists[1]),np.nanmean(lists[0]),filter_time_length,n_filters,conv_stride,\n",
    "        pool_time_length,pool_stride,filter_length, n_CNN_layer,gruDim,gruLayer,drop_out,])\n",
    "    if verbose:\n",
    "        print(a)\n",
    "    reporterList.append(a)\n",
    "    #print(reporterList)\n",
    "    return(np.nanmean(lists[1])+np.nanmean(lists[0]))\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "print(hyperopt_syllable(is_timespat=False,\n",
    "                        conv_stride=23,\n",
    "                        n_filters=64,\n",
    "                        pool_time_length=3,\n",
    "                        pool_stride=3,\n",
    "                        filter_length=3,\n",
    "                        gruDim=64))\n",
    "'''\n",
    "from hyperopt import hp,STATUS_OK,Trials,fmin,tpe\n",
    "def hyperopt_train(params):\n",
    "    loss=hyperopt_syllable(**params)\n",
    "    return loss\n",
    "\n",
    "hyperparas = {\n",
    "    #'batch_size': hp.choice('batch_size', [4,8]),\n",
    "    #'lr': hp.choice('lr', [0.0005,0.0001]),\n",
    "    'filter_time_length':hp.choice('filter_time_length', [4,5,6]),\n",
    "    'n_filters':hp.choice('n_filters', [256,512,1024,2048]),\n",
    "    'conv_stride': hp.choice('conv_stride', [1,2,3]),\n",
    "    'pool_time_length': hp.choice('pool_time_length', [2,3,4]),\n",
    "    'pool_stride': hp.choice('pool_stride', [1,2,3]),\n",
    "    'filter_length': hp.choice('filter_length', [2,3,4]),\n",
    "    'n_CNN_layer':hp.choice('n_CNN_layer',[1,2,3]),\n",
    "    'gruDim':hp.choice('gruDim', [64,128,256,512]),\n",
    "    'gruLayer':hp.choice('gruLayer',[1,2,3,4]),\n",
    "    'drop_out':hp.choice('drop_out', [0.2,0.3,0.4,0.5,0.6,0.7,0.8]),\n",
    "    #'drop_out':hp.uniform('drop_out', 0.2, 0.8),\n",
    "}\n",
    "\n",
    "\n",
    "def f(params):\n",
    "    try:\n",
    "        loss = hyperopt_train(params)\n",
    "    except Exception as e:\n",
    "        # Handle the exception here (e.g., print the error message)\n",
    "        print(f\"Error: {e}\")\n",
    "        # Assign a high loss value to discourage selecting this parameter combination\n",
    "        return {'loss': 9999, 'status': STATUS_OK}\n",
    "    \n",
    "    return {'loss': loss, 'status': STATUS_OK}\n",
    "\n",
    "trials=Trials()\n",
    "best=fmin(f,hyperparas,algo=tpe.suggest,max_evals=500,trials=trials)\n",
    "print('best',best)\n",
    "\n",
    "'''use best to index the best parameters'''"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b9ea9ba5ae74b1398a630feb3651b8acb263e18aa2ea7db4a76df02121bff196"
  },
  "kernelspec": {
   "display_name": "Python 3.7.16",
   "language": "python",
   "name": "python3.7.16"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "toc-showcode": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
